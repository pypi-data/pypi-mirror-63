import copy

import numpy
from plangym import AtariEnvironment, ParallelEnvironment
from plangym.env import Environment

from fragile.core.env import DiscreteEnv
from fragile.core.states import StatesEnv, StatesModel
from fragile.core.swarm import Swarm
from fragile.core.utils import StateDict


def get_plangym_env(swarm: Swarm) -> AtariEnvironment:
    """Return the :class:`plangym.AtariEnvironment` of the target Swarm."""
    if isinstance(swarm.env, DiscreteEnv):
        if not isinstance(swarm.env._env, Environment):
            raise TypeError("swarm.env needs to represent an Atari game.")
    elif not isinstance(swarm.env, AtariEnv):
        raise TypeError("swarm.env needs to represent an Atari game.")
    plangym_env = swarm.env._env
    if isinstance(plangym_env, ParallelEnvironment):
        return plangym_env._env
    else:
        return plangym_env


class AtariEnv(DiscreteEnv):
    """The AtariEnv acts as an interface with `plangym.AtariEnvironment`.

    It can interact with any Atari environment that follows the interface of ``plangym``.
    """

    STATE_CLASS = StatesEnv

    def get_params_dict(self) -> StateDict:
        """Return a dictionary containing the param_dict to build an instance \
        of States that can handle all the data generated by the environment.
        """
        super_params = super(AtariEnv, self).get_params_dict()
        params = {"game_ends": {"dtype": numpy.bool_}}
        params.update(super_params)
        return params

    def step(self, model_states: StatesModel, env_states: StatesEnv) -> StatesEnv:
        """
        Set the environment to the target states by applying the specified \
        actions an arbitrary number of time steps.

        Args:
            model_states: States representing the data to be used to act on the environment.
            env_states: States representing the data to be set in the environment.

        Returns:
            States containing the information that describes the new state of the Environment.

        """
        actions = model_states.actions.astype(numpy.int32)
        n_repeat_actions = model_states.dt if hasattr(model_states, "dt") else 1
        new_states, observs, rewards, ends, infos = self._env.step_batch(
            actions=actions, states=env_states.states, n_repeat_action=n_repeat_actions
        )
        game_ends = [inf.get("game_end", False) for inf in infos]

        new_state = self.states_from_data(
            states=new_states,
            observs=observs,
            rewards=rewards,
            ends=ends,
            batch_size=len(actions),
            game_ends=game_ends,
        )
        return new_state

    def reset(self, batch_size: int = 1, **kwargs) -> StatesEnv:
        """
        Reset the environment to the start of a new episode and returns a new \
        :class:`StatesEnv` instance describing the state of the :class:`AtariEnvironment`.

        Args:
            batch_size: Number of walkers of the returned state.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            :class:`StatesEnv` instance describing the state of the Environment. \
            The first dimension of the data tensors (number of walkers) will be \
            equal to batch_size.

        """
        state, obs = self._env.reset()
        states = numpy.array([copy.deepcopy(state) for _ in range(batch_size)])
        observs = numpy.array([copy.deepcopy(obs) for _ in range(batch_size)])
        rewards = numpy.zeros(batch_size, dtype=numpy.float32)
        ends = numpy.zeros(batch_size, dtype=numpy.bool_)
        game_ends = numpy.zeros(batch_size, dtype=numpy.bool_)
        new_states = self.states_from_data(
            states=states,
            observs=observs,
            rewards=rewards,
            ends=ends,
            batch_size=batch_size,
            game_ends=game_ends,
        )
        return new_states
