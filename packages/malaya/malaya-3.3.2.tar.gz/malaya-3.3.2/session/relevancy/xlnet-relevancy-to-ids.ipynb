{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro_utils import preprocess_text, encode_ids, encode_pieces\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load('xlnet-base/sp10m.cased.v5.model')\n",
    "\n",
    "with open('xlnet-base/sp10m.cased.v5.vocab') as fopen:\n",
    "    v = fopen.read().split('\\n')[:-1]\n",
    "v = [i.split('\\t') for i in v]\n",
    "v = {i[0]: i[1] for i in v}\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, v):\n",
    "        self.vocab = v\n",
    "        pass\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        return encode_pieces(sp_model, string, return_unicode=False, sample=False)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [sp_model.PieceToId(piece) for piece in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [sp_model.IdToPiece(i) for i in ids]\n",
    "    \n",
    "tokenizer = Tokenizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../relevant-dataset.pkl', 'rb') as fopen:\n",
    "    data = pickle.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def cleaning(string):\n",
    "    string = unidecode(string)\n",
    "    string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip().split()\n",
    "    string = [w for w in string if w[0] != '@']\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro_utils import preprocess_text, encode_ids\n",
    "\n",
    "def tokenize_fn(text):\n",
    "    text = preprocess_text(text, lower= False)\n",
    "    return encode_ids(sp_model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data['train_X']\n",
    "train_Y = data['train_Y']\n",
    "test_X = data['test_X']\n",
    "test_Y = data['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135615/135615 [09:43<00:00, 232.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "SEG_ID_A   = 0\n",
    "SEG_ID_B   = 1\n",
    "SEG_ID_CLS = 2\n",
    "SEG_ID_SEP = 3\n",
    "SEG_ID_PAD = 4\n",
    "MAX_SEQ_LENGTH = 500\n",
    "\n",
    "special_symbols = {\n",
    "    \"<unk>\"  : 0,\n",
    "    \"<s>\"    : 1,\n",
    "    \"</s>\"   : 2,\n",
    "    \"<cls>\"  : 3,\n",
    "    \"<sep>\"  : 4,\n",
    "    \"<pad>\"  : 5,\n",
    "    \"<mask>\" : 6,\n",
    "    \"<eod>\"  : 7,\n",
    "    \"<eop>\"  : 8,\n",
    "}\n",
    "\n",
    "VOCAB_SIZE = 32000\n",
    "UNK_ID = special_symbols[\"<unk>\"]\n",
    "CLS_ID = special_symbols[\"<cls>\"]\n",
    "SEP_ID = special_symbols[\"<sep>\"]\n",
    "MASK_ID = special_symbols[\"<mask>\"]\n",
    "EOD_ID = special_symbols[\"<eod>\"]\n",
    "\n",
    "input_ids, input_masks, segment_ids = [], [], []\n",
    "\n",
    "for text in tqdm(train_X):\n",
    "    tokens_a = tokenize_fn(text)\n",
    "    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n",
    "        tokens_a = tokens_a[:(MAX_SEQ_LENGTH - 2)]\n",
    "        \n",
    "    tokens = []\n",
    "    segment_id = []\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_id.append(SEG_ID_A)\n",
    "    tokens.append(SEP_ID)\n",
    "    segment_id.append(SEG_ID_A)\n",
    "    tokens.append(CLS_ID)\n",
    "    segment_id.append(SEG_ID_CLS)\n",
    "    \n",
    "    input_id = tokens\n",
    "    input_mask = [0] * len(input_id)\n",
    "    if len(input_id) < MAX_SEQ_LENGTH:\n",
    "        delta_len = MAX_SEQ_LENGTH - len(input_id)\n",
    "        input_id = [0] * delta_len + input_id\n",
    "        input_mask = [1] * delta_len + input_mask\n",
    "        segment_id = [SEG_ID_PAD] * delta_len + segment_id\n",
    "    \n",
    "    input_ids.append(input_id)\n",
    "    input_masks.append(input_mask)\n",
    "    segment_ids.append(segment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8405/8405 [00:35<00:00, 238.83it/s]\n"
     ]
    }
   ],
   "source": [
    "test_input_ids, test_input_masks, test_segment_ids = [], [], []\n",
    "\n",
    "for text in tqdm(test_X):\n",
    "    tokens_a = tokenize_fn(text)\n",
    "    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n",
    "        tokens_a = tokens_a[:(MAX_SEQ_LENGTH - 2)]\n",
    "        \n",
    "    tokens = []\n",
    "    segment_id = []\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_id.append(SEG_ID_A)\n",
    "    tokens.append(SEP_ID)\n",
    "    segment_id.append(SEG_ID_A)\n",
    "    tokens.append(CLS_ID)\n",
    "    segment_id.append(SEG_ID_CLS)\n",
    "    \n",
    "    input_id = tokens\n",
    "    input_mask = [0] * len(input_id)\n",
    "    if len(input_id) < MAX_SEQ_LENGTH:\n",
    "        delta_len = MAX_SEQ_LENGTH - len(input_id)\n",
    "        input_id = [0] * delta_len + input_id\n",
    "        input_mask = [1] * delta_len + input_mask\n",
    "        segment_id = [SEG_ID_PAD] * delta_len + segment_id\n",
    "    \n",
    "    test_input_ids.append(input_id)\n",
    "    test_input_masks.append(input_mask)\n",
    "    test_segment_ids.append(segment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('relevancy-ids.pkl', 'wb') as fopen:\n",
    "    pickle.dump({'train_X': input_ids, 'train_Y': train_Y,\n",
    "                 'train_masks': input_masks, 'train_segments': segment_ids,\n",
    "                'test_X': test_input_ids, \n",
    "                 'test_masks': test_input_masks,\n",
    "                 'test_segment_ids': test_segment_ids,\n",
    "                'test_Y': test_Y}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
