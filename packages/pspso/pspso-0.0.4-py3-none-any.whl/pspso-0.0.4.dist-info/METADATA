Metadata-Version: 2.1
Name: pspso
Version: 0.0.4
Summary: pspso is a python package for selecting machine learning algorithms parameters.
Home-page: https://github.com/ayhaidar/pspso
Author: Ali Haidar
Author-email: ali.hdrv@outlook.com
License: MIT
Platform: UNKNOWN
Classifier: Development Status :: 4 - Beta
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Intended Audience :: Developers
Requires-Python: >=3.5
Description-Content-Type: text/markdown
Requires-Dist: numpy (==1.16.1)
Requires-Dist: lightgbm
Requires-Dist: xgboost
Requires-Dist: scikit-learn (>=0.21.2)
Requires-Dist: keras
Requires-Dist: pyswarms (>=1.0.2)
Requires-Dist: matplotlib (>=3.1.1)

# pspso

pspso is a python library for selecting machine learning algorithms parameters.
The first version supports two single algorithms: Multi-Layer Perceptron (MLP) and Support Vector Machine (SVM).
It supports two ensembles: Extreme Gradient Boosting (XGBoost) and Gradient Boosting Decision Trees (GBDT).

## Installation

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install pspso.

```bash
pip install pspso
```

## Usage

### MLP Example
pspso is used to select the machine learning algorithms parameters. It is assumed that the user
has already processed and prepared the training and validation datasets. Below is an example for using the 
pso to select the parameters of the MLP. It should be noticed that pspso handles the MLP random weights intialization issue
that may cause losing the best solution in consecutive iterations.
```python
from pspso import pspso
params = {"optimizer":['adam','nadam','sgd','adadelta'],
    "learning_rate":  [0.01,0.2,2],
    "hiddenactivation": ['sigmoid','tanh','relu'],
    "activation": ['sigmoid','tanh','relu']}
task='binary classification'
score='auc'
number_of_particles=10
number_of_iterations=15
p=pspso.pspso('mlp',params,task,score)
p.fitpspso(X_train,Y_train,X_val,Y_val,number_of_particles=number_of_particles,
               number_of_iterations=number_of_iterations)
p.printresults()
```

In this example, four parameters were examined: optimizer, learning_rate, hiddenactivation, and activation.
The number of neurons in the hidden layer was kept as default.

### Details

The user is given the chance to handle some of the default parameters such as the number of epochs. 
The user can modify this by changing a pspso class intance. For e.g., if you need to change the number of epochs from 50 
to 10 for an MLP training:
```python
from pspso import pspso
task='binary classification'
score='auc'
p=pspso.pspso('mlp',params,task,score)
p.defaultparams['epochs']=10

```

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License
[MIT](https://choosealicense.com/licenses/mit/)

