{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Translation\n",
        "\n",
        "Source code of this tutorial is in [book/machine_translation](https://github.com/PaddlePaddle/book/tree/develop/08.machine_translation). For users new to Paddle book, please refer to [the user guide of Book Documentation](https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#run_the_book).\n",
        "\n",
        "##  Background\n",
        "\n",
        "Machine translation is to translate different languages with computer. The language to be translated is usually called source language, and the language representing the result of translation is called target language. Machine translation is the process of transformation from source language to target language, which is an important research assignment of Natural Language Processing.\n",
        "\n",
        "Machine translation systems at early age were mostly rule-based translation system, which needs linguists make transformation rule between two languages and then input these rules into computer. This method requires proficient  professional linguistic background, but it is hard to cover all rules of a language, let it alone two or more languages. Therefore, the major challenge of traditional machine translation method is the impossibility of a completest set of rules\\[[1](#References)\\].\n",
        "\n",
        "To solve the problem mentioned above, Statistical Machine Translation technology emerged afterwards. For Statistical Machine Translation, transformation rules are automatically learned from a large scale corpus instead of handcrafted rule. So it tackles with the limit of obtaining knowledge in rule-based machine translation systems. However, it still faces certain challenges: 1. man-made feature can never cover all language phenomena. 2. it is hard to use global feature. 3. it depends on many pre-processing parts, such as Word Alignment, Tokenization, Rule Extraction, Parsing. Errors accumulated by those parts will have a great influence on translation.\n",
        "\n",
        "In recent years, Deep Learning technology proposes new solutions to overcome the bottleneck. Two methods for machine translation are realized with the aid of deep learning. 1. Based on the framework of statistical machine translation system, the neural network is in place to improve core parts, such as language model, reordering model and so on (See the left part in figure One). 2. Abandoning the framework of statistical machine translation system, it directly uses neural network to transform source language to target language, which is End-to-End Neural Machine Translation (See right part in figure One), NMT model in short.\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/nmt.png?raw=true\" width = \"400\" align=center/\u003e\u003cbr/\u003e\n",
        "Figure One. Neural Network Machine Translation System\n",
        "\u003c/div\u003e\n",
        "\n",
        "In the following parts, we'll guide you through NMT model and its hands-on implementation in PaddlePaddle\n",
        "\n",
        "## Result Demo\n",
        "\n",
        "Take Chinese to English translation model as an example. For a trained model, if input the following tokenized Chinese sentence :\n",
        "\n",
        "```text\n",
        "这些 是 希望 的 曙光 和 解脱 的 迹象 .\n",
        "```\n",
        "\n",
        "If it sets the entries of translation result ( e.t. the width of [beam search algorithm](#beam search algorithm)) as 3, the generated English sentence is as follows:\n",
        "\n",
        "```text\n",
        "0 -5.36816   These are signs of hope and relief . \u003ce\u003e\n",
        "1 -6.23177   These are the light of hope and relief . \u003ce\u003e\n",
        "2 -7.7914  These are the light of hope and the relief of hope . \u003ce\u003e\n",
        "```\n",
        "\n",
        "- The first column to the left is the serial numbers of generated sentences. The second column from left is scores of the sentences in descending order, in which higher score is better. The third column contains the generated English sentences.\n",
        "\n",
        "- In addition, there are two special marks. One is `\u003ce\u003e`, indicating the end of a sentence and another one is `\u003cunk\u003e`, representing unknown word, which have never appeared in dictionary.\n",
        "\n",
        "## Exploration of Models\n",
        "\n",
        "In this section, let's scrutinize Bi-directional Recurrent Neural Network, typical Encoder-Decoder structure in NMT model and beam search algorithm.\n",
        "\n",
        "### Bi-directional Recurrent Neural Network\n",
        "\n",
        "We have introduced a bi-directional recurrent neural network in the chapter [label_semantic_roles](https://github.com/PaddlePaddle/book/blob/develop/07.label_semantic_roles/README.md). Here we introduce another network proposed by Bengio team in thesis \\[[2](#References),[4](#References)\\] The aim of this network is to input a sequence and get its features at each time step. Specifically, fixed-length vector is incorporated to represent contextual semantic information for each time step in the output.\n",
        "\n",
        "To be concrete, the Bi-directional recurrent neural network sequentially processes the input sequences in time dimension in sequential order or in reverse order, i.e., forward and backward. And the output of RNN at each time step are concatenated to be the final output layer. Hereby the output node of each time step contains complete past and future context information of current time step of input sequence. The figure below shows a bi-directional recurrent neural network expanded by time step. The network consists of a forward and a backward RNN with six weight matrices: a weight matrix ($W_1, W_3$) from input layer to the forward and backward hidden layers, and a weight matrix ($W_2, W_5$) from a hidden layer to itself (self-loop), the weight matrix from the forward hidden layer and the backward hidden layer to the output layer ($W_4, W_6$). Note that there is no connection between the forward hidden layer and the backward hidden layer.\n",
        "\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src = \"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/bi_rnn.png?raw=true\" width=\"400\"\u003e\u003cbr/\u003e\n",
        "Figure 2. Bi-directional Recurrent Neural Network expanded by time step.\n",
        "\u003c/div\u003e\n",
        "\n",
        "### Encoder-Decoder Structure\n",
        "\n",
        "The Encoder-Decoder\\[[2](#References)\\] structure helps transform a source sequence with arbitrary length to another target sequence with arbitrary length. In the encoding phase, it encodes the entire source sequence into a vector. And in the decoding phase, it decodes the entire target sequence by maximizing the predicted sequence probability. The encoding and decoding process is usually implemented by RNN.\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src =\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/encoder_decoder.png?raw=true\" width=\"700\"\u003e\u003cbr/\u003e\n",
        "Figure 3. Encoder-Decoder Frame\n",
        "\u003c/div\u003e\n",
        "\n",
        "\u003ca name=\"encoder\"\u003e\u003c/a\u003e\n",
        "#### Encoder\n",
        "\n",
        "Encoding can be done in 3 steps:\n",
        "\n",
        "1.One-hot vector : each word $x_i$ of the source language sentence $x=\\left \\{ x_1,x_2,...,x_T \\right \\}$ is represented as a column vector $w_i\\epsilon \\left \\{ 0,1 \\right \\}^{\\left | V \\right |},i=1,2,...,T$. The dimension of this vector $w_i$ is the same as the vocabulary size $\\left | V \\right |$, and there is only one dimension with a value of 1 (the position corresponds to the position of the word in the vocabulary), and the rest are all zero.\n",
        "\n",
        "2.Word vector mapped to low-dimensional semantic space: one-hot vector indicates that there are two problems: 1. the dimension of generated vector is often large, which is easy to cause dimension disaster. 2. it is difficult to characterize the relationship between words and words (such as semantical similarity, that is, the semantics cannot be expressed well.) Therefore, one-hot vector needs to be mapped to a low-dimensional semantic space, represented by a dense vector(called a word vector) with fixed dimensions . The mapping matrix is $C\\epsilon R^{K\\times \\left | V \\right |}$, with $s_i=Cw_i$ representing the word vector of the $i$ word, $K$ representing the dimension of vector.\n",
        "\n",
        "3.Encode a source language word sequence with RNN: The calculation for this process is $h_i=\\varnothing _\\theta \\left ( h_{i-1}, s_i \\right )$, where $h_0$ is an all-zero vector. $\\varnothing _\\theta$ is a non-linear activation function, and the result $\\mathbf{h}=\\left \\{ h_1,..., h_T \\right \\}$ is the state code sequence of source language $T$ words read by RNN respectively. The vector representation of the entire sentence can use $\\mathbf{h}$ in the state code of the last time step $T$, or use the pooling result in the time dimension.\n",
        "\n",
        "Step 3 can also use bi-directional recurrent neural network to implement more complex sentence-coded representation, which can be implemented with bi-directional GRU. The forward GRU sequentially encodes the source language word  in the order of the word sequence $(x_1, x_2,..., x_T)$, and obtains a series of hidden layer states $(\\overrightarrow{h_1},\\overrightarrow{h_2},. ..,\\overrightarrow{h_T})$. Similarly, the backward GRU encodes the source language word in the order of $(x_T,x_{T-1},...,x_1)$, resulting in $(\\overleftarrow{h_1},\\overleftarrow{h_2},. ..,\\overleftarrow{h_T})$. Finally, for the word $x_i$, the hidden layer state is obtained by jointing the two GRUs, namely $h_i=\\left [ \\overrightarrow{h_i^T},\\overleftarrow{h_i^T} \\right ]^{T} $.\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/encoder_attention.png?raw=true\" width=\"400\"\u003e\u003cbr/\u003e\n",
        "Figure 4. Use bi-directional GRU encoder\n",
        "\u003c/div\u003e\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "During the training of machine translation tasks, the goal of in decode period is to maximize the probability of the next correct target language word. The idea is:\n",
        "1.At each time, according to the encoding information of the source language sentence (also called context vector) $c$, the $i$th word $u_i$ of the real target language sequence and the hidden layer state $z_i $ of RNN at $i$th time, it calculates the next hidden layer state $z_{i+1}$. The formula is as follows:\n",
        "$$z_{i+1}=\\phi_{\\theta '} \\left ( c,u_i,z_i \\right )$$\n",
        "Where $\\phi _{\\theta '}$ is a nonlinear activation function; $c$ is the context vector of the source language sentence, if the attention mechanism is not used and the output of [encoder](#encoder) is the last element after the source language sentence is encoded, you can define $c=h_T$; $u_i$ is the $i$th word of the target language sequence, and $u_0$ is the start tag `\u003cs\u003e` of the target language sequence, indicating the start of decoding; $z_i$ is the hidden layer state of the RNN at $i$th time, and $z_0$ is an all-zero vector.\n",
        "\n",
        "2.Normalize $z_{i+1}$ by `softmax` to get the probability distribution $p_{i+1}$ of the $i+1$th word of the target language sequence. The probability distribution formula is as follows:\n",
        "$$p\\left ( u_{i+1}|u_{\u0026lt;i+1},\\mathbf{x} \\right )=softmax(W_sz_{i+1}+b_z)$$\n",
        "Where $W_sz_{i+1}+b_z$ scores each possible output word and normalizes with softmax to get the probability $p_{i+1}$ of $i+1$th word.\n",
        "\n",
        "3.Calculate the cost according to $p_{i+1}$ and $u_{i+1}$.\n",
        "\n",
        "4.Repeat steps 1~3 until all words in the target language sequence have been processed.\n",
        "\n",
        "The process of generating machine translation tasks, in general, is to translate source language sentences based on pre-trained model. The decode period in the generation process is different from the training process above. For details, please refer to [Beam Search Algorithm](#Beam Search Algorithm).\n",
        "\n",
        "\u003ca name=\"beam search algorithm\"\u003e\u003c/a\u003e\n",
        "### Beam Search Algorithm\n",
        "\n",
        "Beam Search ([beam search](http://en.wikipedia.org/wiki/Beam_search)) is a heuristic graph search algorithm for searching the graph or tree for the optimal extended nodes in a finite set, usually used in systems with very large solution space (such as machine translation, speech recognition), for that the memory can't fit all the unfolded solutions in the graph or tree. If you want to translate \"`\u003cs\u003eHello\u003ce\u003e`\" in the machine translation task, even if there are only 3 words (`\u003cs\u003e`, `\u003ce\u003e`, `hello`) in the target language dictionary, it is possible generate infinite sentences (the number of occurrences of `hello`  is uncertain). In order to find better translation results, we can use beam search algorithm.\n",
        "\n",
        "The beam search algorithm uses a breadth-first strategy to build a search tree. At each level of the tree, the nodes are sorted according to the heuristic cost (in this tutorial, the sum of the log probabilities of the generated words), and then only the predetermined number of nodes (commonly referred to in the literature as beam width, beam size, 柱宽度, etc.). Only these nodes will continue to expand in the next layer, and other nodes will be cut off, that is, the nodes with higher quality are retained, and the nodes with poor quality are pruned. Therefore, the space and time occupied by the search are greatly reduced, but the disadvantage is that there is no guarantee that an optimal solution will be obtained.\n",
        "\n",
        "In the decode period of using beam search algorithm, the goal is to maximize the probability of generated sequence. The idea is:\n",
        "1.At each time, the next hidden layer state $z_{i+1}$ is calculated according to the encoding information $c$ of the source language sentence, the generated $i$th target language sequence words $u_i$, and the hidden layer state $z_i$ of RNN at $i$th time.\n",
        "\n",
        "2.Normalize $z_{i+1}$ by `softmax` to get the probability distribution $p_{i+1}$ of the $i+1$th words of the target language sequence.\n",
        "\n",
        "3.The word $u_{i+1}$ is sampled according to $p_{i+1}$.\n",
        "\n",
        "4.Repeat steps 1~3 until you get the sentence end tag `\u003ce\u003e` or exceed the maximum generation length of the sentence.\n",
        "\n",
        "Note: The formula for $z_{i+1}$ and $p_{i+1}$ is the same as in [Decoder](#Decoder). And since each step of the generation is implemented by the greedy method, it is not guaranteed to obtain the global optimal solution.\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "This tutorial uses [bitexts(after selection)] in the [WMT-14](http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/) dataset (http://www-lium.univ- Lemans.fr/~schwenk/cslm_joint_paper/data/bitexts.tgz) as a training set, [dev+test data](http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/data/dev+test.tgz) as a test set and generated set.\n",
        "\n",
        "### Data Pre-processing\n",
        "\n",
        "It contains two steps in pre-processing:\n",
        "\n",
        "-Merge parallel corpora files from source language to target language into one file:\n",
        "\n",
        "-Merge every `XXX.src` and `XXX.trg` into one file as `XXX`.\n",
        "\n",
        "-Content in $i$th row of `XXX` is the connection of $i$th row of `XXX.src` with $i$th row of `XXX.trg`, which is divided by '\\t'.\n",
        "\n",
        "-Create source language dictionary and target language dictionary of train data. There are **DICTSIZE** words in each dictionary, including:  (DICTSIZE - 3) words with highest frequency in the corpus, and 3 special symbols `\u003cs\u003e` (the beginning of the sequence), `\u003ce\u003e` ( the end of the sequence) and `\u003cunk\u003e` (unknown word).\n",
        "\n",
        "### Sample Data\n",
        "\n",
        "Because the data volume of the complete data set is large, in order to verify the training process, the PaddlePaddle interface paddle.data set.wmt14 provides a pre-processed [smaller scale dataset](http://paddlepaddle.bj.bcebos.com/demo/wmt_shrinked_data/wmt14.tgz) by default .\n",
        "\n",
        "In the data set, there are 193,319 training data, 6003 test data, and a dictionary with length of 30,000. Due to the limit of data size, the effects of models trained with this dataset are not guaranteed.\n",
        "\n",
        "## Model Configuration\n",
        "\n",
        "Next we start configuring model according to input data. First we import necessary library functions and define global variables.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from __future__ import print_function\n",
        "import contextlib\n",
        "\n",
        "import numpy as np\n",
        "import paddle\n",
        "import paddle.fluid as fluid\n",
        "import paddle.fluid.framework as framework\n",
        "import paddle.fluid.layers as pd\n",
        "from paddle.fluid.executor import Executor\n",
        "from functools import partial\n",
        "import os\n",
        "try:\n",
        "    from paddle.fluid.contrib.trainer import *\n",
        "    from paddle.fluid.contrib.inferencer import *\n",
        "except ImportError:\n",
        "    print(\n",
        "        \"In the fluid 1.0, the trainer and inferencer are moving to paddle.fluid.contrib\",\n",
        "        file=sys.stderr)\n",
        "    from paddle.fluid.trainer import *\n",
        "    from paddle.fluid.inferencer import *\n",
        "\n",
        "dict_size = 30000 # dictionary dimension\n",
        "source_dict_dim = target_dict_dim = dict_size # source/target language dictionary dimension\n",
        "hidden_dim = 32 # size of hidden layer in encoder\n",
        "word_dim = 16 # dimension of word vector\n",
        "batch_size = 2 # the number of samples in batch\n",
        "max_length = 8 # the maximum length of generated sentence\n",
        "beam_size = 2  # width of beam\n",
        "\n",
        "decoder_size = hidden_dim # size of hidden layer in decoder\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Then the frame of encoder is implemented as follows:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "   def encoder(is_sparse):\n",
        "    # define input data id of source language\n",
        "    src_word_id = pd.data(\n",
        "        name=\"src_word_id\", shape=[1], dtype='int64', lod_level=1)\n",
        "    # reflect encode above on the word vector of low-dimension language space.\n",
        "    src_embedding = pd.embedding(\n",
        "        input=src_word_id,\n",
        "        size=[dict_size, word_dim],\n",
        "        dtype='float32',\n",
        "        is_sparse=is_sparse,\n",
        "        param_attr=fluid.ParamAttr(name='vemb'))\n",
        "    # LSTM layer：fc + dynamic_lstm\n",
        "    fc1 = pd.fc(input=src_embedding, size=hidden_dim * 4, act='tanh')\n",
        "    lstm_hidden0, lstm_0 = pd.dynamic_lstm(input=fc1, size=hidden_dim * 4)\n",
        "    # Fetch the final state after the sequence encode of source language\n",
        "    encoder_out = pd.sequence_last_step(input=lstm_hidden0)\n",
        "    return encoder_out\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "Then implement decoder in training mode:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "   def train_decoder(context, is_sparse):\n",
        "    # Define input data of sequence id of target language and reflect it on word vector of low-dimension language space\n",
        "    trg_language_word = pd.data(\n",
        "        name=\"target_language_word\", shape=[1], dtype='int64', lod_level=1)\n",
        "    trg_embedding = pd.embedding(\n",
        "        input=trg_language_word,\n",
        "        size=[dict_size, word_dim],\n",
        "        dtype='float32',\n",
        "        is_sparse=is_sparse,\n",
        "        param_attr=fluid.ParamAttr(name='vemb'))\n",
        "\n",
        "    rnn = pd.DynamicRNN()\n",
        "    with rnn.block(): # use DynamicRNN to define computation at each step\n",
        "        # Fetch input word vector of target language at present step\n",
        "        current_word = rnn.step_input(trg_embedding)\n",
        "        # obtain state of hidden layer\n",
        "        pre_state = rnn.memory(init=context)\n",
        "        # computing unit of decoder: single-layer forward network\n",
        "        current_state = pd.fc(input=[current_word, pre_state],\n",
        "                              size=decoder_size,\n",
        "                              act='tanh')\n",
        "        # compute predicting probability of nomarlized word\n",
        "        current_score = pd.fc(input=current_state,\n",
        "                              size=target_dict_dim,\n",
        "                              act='softmax')\n",
        "        # update hidden layer of RNN\n",
        "        rnn.update_memory(pre_state, current_state)\n",
        "        # output predicted probability\n",
        "        rnn.output(current_score)\n",
        "\n",
        "    return rnn()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "implement decoder in inference mode\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def decode(context, is_sparse):\n",
        "    init_state = context\n",
        "    # define counter variable in the decoding\n",
        "    array_len = pd.fill_constant(shape=[1], dtype='int64', value=max_length)\n",
        "    counter = pd.zeros(shape=[1], dtype='int64', force_cpu=True)\n",
        "\n",
        "    # define tensor array to save content at each time step, and write initial id, score and state\n",
        "    state_array = pd.create_array('float32')\n",
        "    pd.array_write(init_state, array=state_array, i=counter)\n",
        "\n",
        "    ids_array = pd.create_array('int64')\n",
        "    scores_array = pd.create_array('float32')\n",
        "\n",
        "    init_ids = pd.data(name=\"init_ids\", shape=[1], dtype=\"int64\", lod_level=2)\n",
        "    init_scores = pd.data(\n",
        "        name=\"init_scores\", shape=[1], dtype=\"float32\", lod_level=2)\n",
        "\n",
        "    pd.array_write(init_ids, array=ids_array, i=counter)\n",
        "    pd.array_write(init_scores, array=scores_array, i=counter)\n",
        "\n",
        "    # define conditional variable to stop loop\n",
        "    cond = pd.less_than(x=counter, y=array_len)\n",
        "    # define while_op\n",
        "    while_op = pd.While(cond=cond)\n",
        "    with while_op.block(): # define the computing of each step\n",
        "        # obtain input at present step of decoder, including id chosen at previous step, corresponding score and state at previous step.\n",
        "        pre_ids = pd.array_read(array=ids_array, i=counter)\n",
        "        pre_state = pd.array_read(array=state_array, i=counter)\n",
        "        pre_score = pd.array_read(array=scores_array, i=counter)\n",
        "\n",
        "        # update input state as state correspondent with id chosen at previous step\n",
        "        pre_state_expanded = pd.sequence_expand(pre_state, pre_score)\n",
        "        # computing logic of decoder under the same train mode, including input vector and computing unit of decoder\n",
        "        # compute predicting probability of normalized word\n",
        "        pre_ids_emb = pd.embedding(\n",
        "            input=pre_ids,\n",
        "            size=[dict_size, word_dim],\n",
        "            dtype='float32',\n",
        "            is_sparse=is_sparse)\n",
        "        current_state = pd.fc(input=[pre_state_expanded, pre_ids_emb],\n",
        "                              size=decoder_size,\n",
        "                              act='tanh')\n",
        "        current_state_with_lod = pd.lod_reset(x=current_state, y=pre_score)\n",
        "        current_score = pd.fc(input=current_state_with_lod,\n",
        "                              size=target_dict_dim,\n",
        "                              act='softmax')\n",
        "        topk_scores, topk_indices = pd.topk(current_score, k=beam_size)\n",
        "\n",
        "        # compute accumulated score and perform beam search\n",
        "        accu_scores = pd.elementwise_add(\n",
        "            x=pd.log(topk_scores), y=pd.reshape(pre_score, shape=[-1]), axis=0)\n",
        "        selected_ids, selected_scores = pd.beam_search(\n",
        "            pre_ids,\n",
        "            pre_score,\n",
        "            topk_indices,\n",
        "            accu_scores,\n",
        "            beam_size,\n",
        "            end_id=10,\n",
        "            level=0)\n",
        "\n",
        "        pd.increment(x=counter, value=1, in_place=True)\n",
        "        # write search result and corresponding hidden layer into tensor array\n",
        "        pd.array_write(current_state, array=state_array, i=counter)\n",
        "        pd.array_write(selected_ids, array=ids_array, i=counter)\n",
        "        pd.array_write(selected_scores, array=scores_array, i=counter)\n",
        "\n",
        "        # update condition to stop loop\n",
        "        length_cond = pd.less_than(x=counter, y=array_len)\n",
        "        finish_cond = pd.logical_not(pd.is_empty(x=selected_ids))\n",
        "        pd.logical_and(x=length_cond, y=finish_cond, out=cond)\n",
        "\n",
        "    translation_ids, translation_scores = pd.beam_search_decode(\n",
        "        ids=ids_array, scores=scores_array, beam_size=beam_size, end_id=10)\n",
        "\n",
        "    return translation_ids, translation_scores\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Furthermore, we define a `train_program` to use result computed by `inference_program` and compute error with the help of marked data. We also define an `optimizer_func` to define optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def train_program(is_sparse):\n",
        "    context = encoder(is_sparse)\n",
        "    rnn_out = train_decoder(context, is_sparse)\n",
        "    label = pd.data(\n",
        "        name=\"target_language_next_word\", shape=[1], dtype='int64', lod_level=1)\n",
        "    cost = pd.cross_entropy(input=rnn_out, label=label)\n",
        "    avg_cost = pd.mean(cost)\n",
        "    return avg_cost\n",
        "\n",
        "\n",
        "def optimizer_func():\n",
        "    return fluid.optimizer.Adagrad(\n",
        "        learning_rate=1e-4,\n",
        "        regularization=fluid.regularizer.L2DecayRegularizer(\n",
        "            regularization_coeff=0.1))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Train Model\n",
        "\n",
        "### Define Training Environment\n",
        "Define your training environment and define the train executed on CPU or on GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "use_cuda = False\n",
        "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Define Data Provider\n",
        "The next step is to define data provider for train and test. Data Provider read data with size of `BATCH_SIZE` `paddle.dataset.wmt.train` will provide data with size of `BATCH_SIZE` after reordering every time. The size of reordering is `buf_size`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "train_reader = paddle.batch(\n",
        "        paddle.reader.shuffle(\n",
        "            paddle.dataset.wmt14.train(dict_size), buf_size=1000),\n",
        "        batch_size=batch_size)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Create Trainer\n",
        "Trainer needs a train program and a train optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "is_sparse = False\n",
        "trainer = Trainer(\n",
        "        train_func=partial(train_program, is_sparse),\n",
        "        place=place,\n",
        "        optimizer_func=optimizer_func)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Provide Data\n",
        "\n",
        "`feed_order` is used to define every generated data and reflecting relationship between `paddle.layer.data`. For example, the first column data generated by `wmt14.train` is correspondent with the feature `src_word_id`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "feed_order = ['src_word_id', 'target_language_word', 'target_language_next_word'\n",
        "]\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Event Handler\n",
        "Call function `event_handler` will be called after the touch of an event defined before. For example, we can examine the loss after the training at each step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def event_handler(event):\n",
        "    if isinstance(event, EndStepEvent):\n",
        "        if event.step % 10 == 0:\n",
        "            print('pass_id=' + str(event.epoch) + ' batch=' + str(event.step))\n",
        "\n",
        "        if event.step == 20:\n",
        "            trainer.stop()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Start Training\n",
        "Finally, we feed in `num_epoch` and other parameters and call `trainer.train` to start training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "EPOCH_NUM = 1\n",
        "\n",
        "trainer.train(\n",
        "        reader=train_reader,\n",
        "        num_epochs=EPOCH_NUM,\n",
        "        event_handler=event_handler,\n",
        "        feed_order=feed_order)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Model Application\n",
        "\n",
        "### Define Decoder Part\n",
        "\n",
        "Use `encoder` and `decoder` function defined above to infer corresponding id and score after the translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "context = encoder(is_sparse)\n",
        "translation_ids, translation_scores = decode(context, is_sparse)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Define Data\n",
        "\n",
        "First we initialize id and score to generate tensor as input data. In this prediction, we use the first record in `wmt14.test` to infer and finally use \"source language dictionary\" and \"target language dictionary\" to output corresponding sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "init_ids_data = np.array([1 for _ in range(batch_size)], dtype='int64')\n",
        "init_scores_data = np.array(\n",
        "    [1. for _ in range(batch_size)], dtype='float32')\n",
        "init_ids_data = init_ids_data.reshape((batch_size, 1))\n",
        "init_scores_data = init_scores_data.reshape((batch_size, 1))\n",
        "init_lod = [1] * batch_size\n",
        "init_lod = [init_lod, init_lod]\n",
        "\n",
        "init_ids = fluid.create_lod_tensor(init_ids_data, init_lod, place)\n",
        "init_scores = fluid.create_lod_tensor(init_scores_data, init_lod, place)\n",
        "\n",
        "test_data = paddle.batch(\n",
        "    paddle.reader.shuffle(\n",
        "        paddle.dataset.wmt14.test(dict_size), buf_size=1000),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "feed_order = ['src_word_id']\n",
        "feed_list = [\n",
        "    framework.default_main_program().global_block().var(var_name)\n",
        "    for var_name in feed_order\n",
        "]\n",
        "feeder = fluid.DataFeeder(feed_list, place)\n",
        "\n",
        "src_dict, trg_dict = paddle.dataset.wmt14.get_dict(dict_size)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Test\n",
        "Now we can start predicting. We need provide corresponding parameters in `feed_order` and run it on `executor` to obtain id and score.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "exe = Executor(place)\n",
        "exe.run(framework.default_startup_program())\n",
        "\n",
        "for data in test_data():\n",
        "    feed_data = map(lambda x: [x[0]], data)\n",
        "    feed_dict = feeder.feed(feed_data)\n",
        "    feed_dict['init_ids'] = init_ids\n",
        "    feed_dict['init_scores'] = init_scores\n",
        "\n",
        "    results = exe.run(\n",
        "        framework.default_main_program(),\n",
        "        feed=feed_dict,\n",
        "        fetch_list=[translation_ids, translation_scores],\n",
        "        return_numpy=False)\n",
        "\n",
        "    result_ids = np.array(results[0])\n",
        "    result_ids_lod = results[0].lod()\n",
        "    result_scores = np.array(results[1])\n",
        "\n",
        "    print(\"Original sentence:\")\n",
        "    print(\" \".join([src_dict[w] for w in feed_data[0][0][1:-1]]))\n",
        "    print(\"Translated score and sentence:\")\n",
        "    for i in xrange(beam_size):\n",
        "        start_pos = result_ids_lod[1][i] + 1\n",
        "        end_pos = result_ids_lod[1][i+1]\n",
        "        print(\"%d\\t%.4f\\t%s\\n\" % (i+1, result_scores[end_pos-1],\n",
        "                \" \".join([trg_dict[w] for w in result_ids[start_pos:end_pos]])))\n",
        "\n",
        "    break\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "End-to-End neural network translation is an recently acclaimed machine translation method. In this section, we introduced the typical Encoder-Decoder of NMT. Because NMT is a typical Seq2Seq (Sequence to Sequence) learning task, tasks of Seq2Seq, such as query rewriting, abstraction, single round dialogue, can be tackled by this model.\n",
        "\n",
        "\u003ca name=\"References\"\u003e\u003c/a\u003e\n",
        "## References\n",
        "\n",
        "1. Koehn P. [Statistical machine translation](https://books.google.com.hk/books?id=4v_Cx1wIMLkC\u0026printsec=frontcover\u0026hl=zh-CN\u0026source=gbs_ge_summary_r\u0026cad=0#v=onepage\u0026q\u0026f=false)[M]. Cambridge University Press, 2009.\n",
        "2. Cho K, Van Merriënboer B, Gulcehre C, et al. [Learning phrase representations using RNN encoder-decoder for statistical machine translation](http://www.aclweb.org/anthology/D/D14/D14-1179.pdf)[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014: 1724-1734.\n",
        "3. Chung J, Gulcehre C, Cho K H, et al. [Empirical evaluation of gated recurrent neural networks on sequence modeling](https://arxiv.org/abs/1412.3555)[J]. arXiv preprint arXiv:1412.3555, 2014.\n",
        "4.  Bahdanau D, Cho K, Bengio Y. [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)[C]//Proceedings of ICLR 2015, 2015.\n",
        "5. Papineni K, Roukos S, Ward T, et al. [BLEU: a method for automatic evaluation of machine translation](http://dl.acm.org/citation.cfm?id=1073135)[C]//Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002: 311-318.\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\n",
        "\u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"\u003e\u003cimg alt=\"知识共享许可协议\" style=\"border-width:0\" src=\"https://paddlepaddleimage.cdn.bcebos.com/bookimage/camo.png\" /\u003e\u003c/a\u003e\u003cbr /\u003e\u003cspan xmlns:dct=\"http://purl.org/dc/terms/\" href=\"http://purl.org/dc/dcmitype/Text\" property=\"dct:title\" rel=\"dct:type\"\u003eThis tutorial\u003c/span\u003e is contributed by \u003ca xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://book.paddlepaddle.org\" property=\"cc:attributionName\" rel=\"cc:attributionURL\"\u003ePaddlePaddle\u003c/a\u003e, and licensed under a \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"\u003eCreative Commons Attribution-ShareAlike 4.0 International License\u003c/a\u003e.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
