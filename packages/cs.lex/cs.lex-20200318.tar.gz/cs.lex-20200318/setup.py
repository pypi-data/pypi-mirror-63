#!/usr/bin/env python
from setuptools import setup
setup(
  name = 'cs.lex',
  description = 'Lexical analysis functions, tokenisers, transcribers.',
  author = 'Cameron Simpson',
  author_email = 'cs@cskk.id.au',
  version = '20200318',
  url = 'https://bitbucket.org/cameron_simpson/css/commits/all',
  classifiers = ['Programming Language :: Python', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 3', 'Development Status :: 4 - Beta', 'Intended Audience :: Developers', 'Operating System :: OS Independent', 'Topic :: Software Development :: Libraries :: Python Modules', 'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)'],
  include_package_data = True,
  install_requires = ['cs.deco', 'cs.py3'],
  keywords = ['python2', 'python3'],
  license = 'GNU General Public License v3 or later (GPLv3+)',
  long_description = '*Latest release 20200318*:\nNew lc_() function to lowercase and dash a string, new titleify_lc() to mostly reverse lc_().\nNew format_as function, FormatableMixin and related FormatAsError.\n\nLexical analysis functions, tokenisers, transcribers.\n\nAn arbitrary assortment of lexical and tokenisation functions useful\nfor writing recursive descent parsers, of which I have several.\nThere are also some transcription function for producing text\nfrom various objects, such as `hexify` and `unctrl`.\n\nGenerally the get_* functions accept a source string and an offset\n(usually optional, default `0`) and return a token and the new offset,\nraising `ValueError` on failed tokenisation.\n\n## Function `as_lines(chunks, partials=None)`\n\nGenerator yielding complete lines from arbitrary pieces of text from\nthe iterable `chunks`.\n\nAfter completion, any remaining newline-free chunks remain\nin the partials list; this will be unavailable to the caller\nunless the list is presupplied.\n\n## Function `cutprefix(s, prefix)`\n\nStrip a `prefix` from the front of `s`.\nReturn the suffix if `.startswith(prefix)`, else `s`.\n\nExample:\n\n    >>> abc_def = \'abc.def\'\n    >>> cutprefix(abc_def, \'abc.\')\n    \'def\'\n    >>> cutprefix(abc_def, \'zzz.\')\n    \'abc.def\'\n    >>> cutprefix(abc_def, \'.zzz\') is abc_def\n    True\n\n## Function `cutsuffix(s, suffix)`\n\nStrip a `suffix` from the end of `s`.\nReturn the prefix if `.endswith(suffix)`, else `s`.\n\nExample:\n\n    >>> abc_def = \'abc.def\'\n    >>> cutsuffix(abc_def, \'.def\')\n    \'abc\'\n    >>> cutsuffix(abc_def, \'.zzz\')\n    \'abc.def\'\n    >>> cutsuffix(abc_def, \'.zzz\') is abc_def\n    True\n\n## Function `format_as(format_s, format_mapping, error_sep=None)`\n\nFormat the string `format_s` using `format_mapping`,\nreturn the formatted result.\nThis is a wrapper for `str.format_map`\nwhich raises a more informative `FormatAsError` exception on failure.\n\nParameters:\n* `format_s`: the format string to use as the template\n* `format_mapping`: the mapping of available replacement fields\n* `error_sep`: optional separator for the multipart error message,\n  default from FormatAsError.DEFAULT_SEPARATOR:\n  `\'; \'`\n\n## Class `FormatableMixin`\n\nA mixin to supply a `format_as` method for classes with an\nexisting `format_kwargs` method.\n\nThe `format_as` method is like an inside out `str.format` or\n`object._format__` method.\n`str.format` is designed for formatting a string from a variety\nof other obejcts supplied in the keyword arguments,\nand `object.__format__` is for filling out a single `str.format`\nreplacement field from a single object.\nBy contrast, `format_as` is designed to fill out an entire format\nstring from the current object.\n\nFor example, the `cs.tagset.TagSet` class\nuses `FormatableMixin` to provide a `format_as` method\nwhose replacement fields are derived from the tags in the tag set.\n\n## Class `FormatAsError(builtins.LookupError,builtins.Exception,builtins.BaseException)`\n\nSubclass of `LookupError` for use by `format_as`.\n\n## Function `get_chars(s, offset, gochars)`\n\nScan the string `s` for characters in `gochars` starting at `offset`.\nReturn `(match,new_offset)`.\n\n## Function `get_decimal(s, offset=0)`\n\nScan the string `s` for decimal characters starting at `offset`.\nReturn `(dec_string,new_offset)`.\n\n## Function `get_decimal_or_float_value(s, offset=0)`\n\nFetch a decimal or basic float (nnn.nnn) value\nfrom the str `s` at `offset`.\nReturn `(value,new_offset)`.\n\n## Function `get_decimal_value(s, offset=0)`\n\nScan the string `s` for a decimal value starting at `offset`.\nReturn `(value,new_offset)`.\n\n## Function `get_delimited(s, offset, delim)`\n\nCollect text from the string `s` from position `offset` up\nto the first occurence of delimiter `delim`; return the text\nexcluding the delimiter and the offset after the delimiter.\n\n## Function `get_dotted_identifier(s, offset=0, **kw)`\n\nScan the string `s` for a dotted identifier (by default an\nASCII letter or underscore followed by letters, digits or\nunderscores) with optional trailing dot and another dotted\nidentifier, starting at `offset` (default `0`).\nReturn `(match,new_offset)`.\n\nNote: the empty string and an unchanged offset will be returned if\nthere is no leading letter/underscore.\n\n## Function `get_envvar(s, offset=0, environ=None, default=None, specials=None)`\n\nParse a simple environment variable reference to $varname or\n$x where "x" is a special character.\n\nParameters:\n* `s`: the string with the variable reference\n* `offset`: the starting point for the reference\n* `default`: default value for missing environment variables;\n   if None (the default) a ValueError is raised\n* `environ`: the environment mapping, default os.environ\n* `specials`: the mapping of special single character variables\n\n## Function `get_hexadecimal(s, offset=0)`\n\nScan the string `s` for hexadecimal characters starting at `offset`.\nReturn `(hex_string,new_offset)`.\n\n## Function `get_hexadecimal_value(s, offset=0)`\n\nScan the string `s` for a hexadecimal value starting at `offset`.\nReturn `(value,new_offset)`.\n\n## Function `get_identifier(s, offset=0, alpha=\'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\', number=\'0123456789\', extras=\'_\')`\n\nScan the string `s` for an identifier (by default an ASCII\nletter or underscore followed by letters, digits or underscores)\nstarting at `offset` (default 0).\nReturn `(match,new_offset)`.\n\n*Note*: the empty string and an unchanged offset will be returned if\nthere is no leading letter/underscore.\n\nParameters:\n* `s`: the string to scan\n* `offset`: the starting offset, default `0`.\n* `alpha`: the characters considered alphabetic,\n  default `string.ascii_letters`.\n* `number`: the characters considered numeric,\n  default `string.digits`.\n* `extras`: extra characters considered part of an identifier,\n  default `\'_\'`.\n\n## Function `get_nonwhite(s, offset=0)`\n\nScan the string `s` for characters not in `string.whitespace`\nstarting at `offset` (default 0).\nReturn `(match,new_offset)`.\n\n## Function `get_other_chars(s, offset=0, stopchars=None)`\n\nScan the string `s` for characters not in `stopchars` starting\nat `offset` (default `0`).\nReturn `(match,new_offset)`.\n\n## Function `get_qstr(s, offset=0, q=\'"\', environ=None, default=None, env_specials=None)`\n\nGet quoted text with slosh escapes and optional environment substitution.\n\nParameters:\n* `s`: the string containg the quoted text.\n* `offset`: the starting point, default 0.\n* `q`: the quote character, default `\'"\'`. If `q` is set to `None`,\n  do not expect the string to be delimited by quote marks.\n* `environ`: if not `None`, also parse and expand $envvar references.\n* `default`: passed to `get_envvar`\n\n## Function `get_qstr_or_identifier(s, offset)`\n\nParse a double quoted string or an identifier.\n\n## Function `get_sloshed_text(s, delim, offset=0, slosh=\'\\\\\', mapper=<function slosh_mapper at 0x10fb968b0>, specials=None)`\n\nCollect slosh escaped text from the string `s` from position\n`offset` (default `0`) and return the decoded unicode string and\nthe offset of the completed parse.\n\nParameters:\n* `delim`: end of string delimiter, such as a single or double quote.\n* `offset`: starting offset within `s`, default `0`.\n* `slosh`: escape character, default a slosh (\'\\\').\n* `mapper`: a mapping function which accepts a single character\n  and returns a replacement string or `None`; this is used the\n  replace things such as \'\\t\' or \'\\n\'. The default is the\n  `slosh_mapper` function, whose default mapping is `SLOSH_CHARMAP`.\n* `specials`: a mapping of other special character sequences and parse\n  functions for gathering them up. When one of the special\n  character sequences is found in the string, the parse\n  function is called to parse at that point.\n  The parse functions accept\n  `s` and the offset of the special character. They return\n  the decoded string and the offset past the parse.\n\nThe escape character `slosh` introduces an encoding of some\nreplacement text whose value depends on the following character.\nIf the following character is:\n* the escape character `slosh`, insert the escape character.\n* the string delimiter `delim`, insert the delimiter.\n* the character \'x\', insert the character with code from the following\n  2 hexadecimal digits.\n* the character \'u\', insert the character with code from the following\n  4 hexadecimal digits.\n* the character \'U\', insert the character with code from the following\n  8 hexadecimal digits.\n* a character from the keys of `mapper`\n\n## Function `get_tokens(s, offset, getters)`\n\nParse the string `s` from position `offset` using the supplied\ntokenise functions `getters`; return the list of tokens matched\nand the final offset.\n\nParameters:\n* `s`: the string to parse.\n* `offset`: the starting position for the parse.\n* `getters`: an iterable of tokeniser specifications.\n\nEach tokeniser specification is either:\n* a callable expecting (s, offset) and returning (token, new_offset)\n* a literal string, to be matched exactly\n* a tuple or list with values (func, args, kwargs);\n  call func(s, offset, *args, **kwargs)\n* an object with a .match method such as a regex;\n  call getter.match(s, offset) and return a match object with\n  a .end() method returning the offset of the end of the match\n\n## Function `get_uc_identifier(s, offset=0, number=\'0123456789\', extras=\'_\')`\n\nScan the string `s` for an identifier as for `get_identifier`,\nbut require the letters to be uppercase.\n\n## Function `get_white(s, offset=0)`\n\nScan the string `s` for characters in `string.whitespace`\nstarting at `offset` (default `0`).\nReturn `(match,new_offset)`.\n\n## Function `hexify(bs)`\n\nA flavour of `binascii.hexlify` returning a `str`.\n\n## Function `htmlify(s, nbsp=False)`\n\nConvert a string for safe transcription in HTML.\n\nParameters:\n* `s`: the string\n* `nbsp`: replaces spaces with `"&nbsp;"` to prevent word folding,\n  default `False`.\n\n## Function `htmlquote(s)`\n\nQuote a string for use in HTML.\n\n## Function `is_dotted_identifier(s, offset=0, **kw)`\n\nTest if the string `s` is an identifier from position `offset` onward.\n\n## Function `is_identifier(s, offset=0, **kw)`\n\nTest if the string `s` is an identifier from position `offset` onward.\n\n## Function `isUC_(s)`\n\nCheck that a string matches `^[A-Z][A-Z_0-9]*$`.\n\n## Function `jsquote(s)`\n\nQuote a string for use in JavaScript.\n\n## Function `lastlinelen(s)`\n\nThe length of text after the last newline in a string.\n\n(Initially used by cs.hier to compute effective text width.)\n\n## Function `lc_(value)`\n\nReturn `value.lower()`\nwith `\'-\'` translated into `\'_\'` and `\' \'` translated into `\'-\'`.\n\nI use this to construct lowercase filenames containing a\nreadable transcription of a title string.\n\nSee also `titleify_lc()`, an imperfect reversal of this.\n\n## Function `match_tokens(s, offset, getters)`\n\nWrapper for get_tokens which catches ValueError exceptions\nand returns (None, offset).\n\n## Function `parseUC_sAttr(attr)`\n\nTake an attribute name and return `(key, is_plural)`.\n\n`\'FOO\'` returns `(`FOO`, False)`.\n`\'FOOs\'` or `\'FOOes\'` returns `(\'FOO\', True)`.\nOtherwise return `(None, False)`.\n\n## Function `phpquote(s)`\n\nQuote a string for use in PHP code.\n\n## Function `skipwhite(s, offset=0)`\n\nConvenience routine for skipping past whitespace;\nreturns the offset of the next nonwhitespace character.\n\n## Function `slosh_mapper(c, charmap=None)`\n\nReturn a string to replace backslash-`c`, or `None`.\n\n## Function `stripped_dedent(s)`\n\nSlightly smarter dedent which ignores a string\'s opening indent.\n\nAlgorithm:\nstrip the supplied string `s`, pull off the leading line,\ndedent the rest, put back the leading line.\n\nThis supports my preferred docstring layout, where the opening\nline of text is on the same line as the opening quote.\n\nExample:\n\n    >>> def func(s):\n    ...   """ Slightly smarter dedent which ignores a string\'s opening indent.\n    ...       Strip the supplied string `s`. Pull off the leading line.\n    ...       Dedent the rest. Put back the leading line.\n    ...   """\n    ...   pass\n    ...\n    >>> from cs.lex import stripped_dedent\n    >>> print(stripped_dedent(func.__doc__))\n    Slightly smarter dedent which ignores a string\'s opening indent.\n    Strip the supplied string `s`. Pull off the leading line.\n    Dedent the rest. Put back the leading line.\n\n## Function `strlist(ary, sep=\', \')`\n\nConvert an iterable to strings and join with ", ".\n\n## Function `tabpadding(padlen, tabsize=8, offset=0)`\n\nCompute some spaces to use a tab padding at an offfset.\n\n## Function `texthexify(bs, shiftin=\'[\', shiftout=\']\', whitelist=None)`\n\nTranscribe the bytes `bs` to text using compact text runs for\nsome common text values.\n\nThis can be reversed with the `untexthexify` function.\n\nThis is an ad doc format devised to be compact but also to\nexpose "text" embedded within to the eye. The original use\ncase was transcribing a binary directory entry format, where\nthe filename parts would be somewhat visible in the transcription.\n\nThe output is a string of hexadecimal digits for the encoded\nbytes except for runs of values from the whitelist, which are\nenclosed in the shiftin and shiftout markers and transcribed\nas is. The default whitelist is values of the ASCII letters,\nthe decimal digits and the punctuation characters \'_-+.,\'.\nThe default shiftin and shiftout markers are \'[\' and \']\'.\n\nString objects converted with either `hexify` and `texthexify`\noutput strings may be freely concatenated and decoded with\n`untexthexify`.\n\nExample:\n\n    >>> texthexify(b\'&^%&^%abcdefghi)(*)(*\')\n    \'265e25265e25[abcdefghi]29282a29282a\'\n\nParameters:\n* `bs`: the bytes to transcribe\n* `shiftin`: Optional. The marker string used to indicate a shift to\n  direct textual transcription of the bytes, default: `\'[\'`.\n* `shiftout`: Optional. The marker string used to indicate a\n  shift from text mode back into hexadecimal transcription,\n  default `\']\'`.\n* `whitelist`: an optional bytes or string object indicating byte\n  values which may be represented directly in text;\n  the default value is the ASCII letters, the decimal digits\n  and the punctuation characters `\'_-+.,\'`.\n\n## Function `titleify_lc(value_lc)`\n\nTranslate `\'-\'` into `\' \'` and `\'_\'` translated into `\'-\'`,\nthen titlecased.\n\nSee also `lc_()`, which this reverses imperfectly.\n\n## Function `unctrl(s, tabsize=8)`\n\nReturn the string `s` with TABs expanded and control characters\nreplaced with printable representations.\n\n## Function `untexthexify(s, shiftin=\'[\', shiftout=\']\')`\n\nDecode a textual representation of binary data into binary data.\n\nThis is the reverse of the `texthexify` function.\n\nOutside of the `shiftin`/`shiftout` markers the binary data\nare represented as hexadecimal. Within the markers the bytes\nhave the values of the ordinals of the characters.\n\nExample:\n\n    >>> untexthexify(\'265e25265e25[abcdefghi]29282a29282a\')\n    b\'&^%&^%abcdefghi)(*)(*\'\n\nParameters:\n* `s`: the string containing the text representation.\n* `shiftin`: Optional. The marker string commencing a sequence\n  of direct text transcription, default `\'[\'`.\n* `shiftout`: Optional. The marker string ending a sequence\n  of direct text transcription, default `\']\'`.\n\n\n\n# Release Log\n\n*Release 20200318*:\nNew lc_() function to lowercase and dash a string, new titleify_lc() to mostly reverse lc_().\nNew format_as function, FormatableMixin and related FormatAsError.\n\n*Release 20200229*:\nNew cutprefix and cutsuffix functions.\n\n*Release 20190812*:\nFix bad slosh escapes in strings.\n\n*Release 20190220*:\nNew function get_qstr_or_identifier.\n\n*Release 20181108*:\nnew function get_decimal_or_float_value to read a decimal or basic float\n\n*Release 20180815*:\nNo semantic changes; update some docstrings and clean some lint, fix a unit test.\n\n*Release 20180810*:\nNew get_decimal_value and get_hexadecimal_value functions.\nNew stripped_dedent function, a slightly smarter textwrap.dedent.\n\n*Release 20171231*:\nNew function get_decimal. Drop unused function dict2js.\n\n*Release 20170904*:\nPython 2/3 ports, move rfc2047 into new cs.rfc2047 module.\n\n*Release 20160828*:\nUse "install_requires" instead of "requires" in DISTINFO.\nDiscard str1(), pointless optimisation.\nunrfc2047: map _ to SPACE, improve exception handling.\nAdd phpquote: quote a string for use in PHP code; add docstring to jsquote.\nAdd is_identifier test.\nAdd get_dotted_identifier.\nAdd is_dotted_identifier.\nAdd get_hexadecimal.\nAdd skipwhite, convenince wrapper for get_white returning just the next offset.\nAssorted bugfixes and improvements.\n\n*Release 20150120*:\ncs.lex: texthexify: backport to python 2 using cs.py3 bytes type\n\n*Release 20150118*:\nmetadata updates\n\n*Release 20150116*:\nPyPI metadata and slight code cleanup.',
  long_description_content_type = 'text/markdown',
  package_dir = {'': 'lib/python'},
  py_modules = ['cs.lex'],
)
