{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 机器翻译\n",
        "\n",
        "本教程源代码目录在[book/machine_translation](https://github.com/PaddlePaddle/book/tree/develop/08.machine_translation),初次使用请您参考[Book文档使用说明](https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#运行这本书)。\n",
        "\n",
        "### 说明\n",
        "1. 硬件要求 本文可支持在CPU、GPU下运行\n",
        "2. 对docker file cuda/cudnn的支持 如果您使用了本文配套的docker镜像，请注意：该镜像对GPU的支持仅限于CUDA 8，cuDNN 5\n",
        "3. 文档中代码和train.py不一致的问题 请注意：为使本文更加易读易用，我们拆分、调整了train.py的代码并放入本文。本文中代码与train.py的运行结果一致，如希望直接看到训练脚本输出效果，可运行[train.py](https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/train.py)。\n",
        "\n",
        "## 背景介绍\n",
        "\n",
        "机器翻译（machine translation, MT）是用计算机来实现不同语言之间翻译的技术。被翻译的语言通常称为源语言（source language），翻译成的结果语言称为目标语言（target language）。机器翻译即实现从源语言到目标语言转换的过程，是自然语言处理的重要研究领域之一。\n",
        "\n",
        "早期机器翻译系统多为基于规则的翻译系统，需要由语言学家编写两种语言之间的转换规则，再将这些规则录入计算机。该方法对语言学家的要求非常高，而且我们几乎无法总结一门语言会用到的所有规则，更何况两种甚至更多的语言。因此，传统机器翻译方法面临的主要挑战是无法得到一个完备的规则集合\\[[1](#参考文献)\\]。\n",
        "\n",
        "为解决以上问题，统计机器翻译（Statistical Machine Translation, SMT）技术应运而生。在统计机器翻译技术中，转化规则是由机器自动从大规模的语料中学习得到的，而非我们人主动提供规则。因此，它克服了基于规则的翻译系统所面临的知识获取瓶颈的问题，但仍然存在许多挑战：1）人为设计许多特征（feature），但永远无法覆盖所有的语言现象；2）难以利用全局的特征；3）依赖于许多预处理环节，如词语对齐、分词或符号化（tokenization）、规则抽取、句法分析等，而每个环节的错误会逐步累积，对翻译的影响也越来越大。\n",
        "\n",
        "近年来，深度学习技术的发展为解决上述挑战提供了新的思路。将深度学习应用于机器翻译任务的方法大致分为两类：1）仍以统计机器翻译系统为框架，只是利用神经网络来改进其中的关键模块，如语言模型、调序模型等（见图1的左半部分）；2）不再以统计机器翻译系统为框架，而是直接用神经网络将源语言映射到目标语言，即端到端的神经网络机器翻译（End-to-End Neural Machine Translation, End-to-End NMT）（见图1的右半部分），简称为NMT模型。\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/nmt.png?raw=true\" width = \"400\" align=center/\u003e\u003cbr/\u003e\n",
        "图1. 基于神经网络的机器翻译系统\n",
        "\u003c/div\u003e\n",
        "\n",
        "本教程主要介绍NMT模型，以及如何用PaddlePaddle来训练一个NMT模型。\n",
        "\n",
        "## 效果展示\n",
        "\n",
        "以中英翻译（中文翻译到英文）的模型为例，当模型训练完毕时，如果输入如下已分词的中文句子：\n",
        "```text\n",
        "这些 是 希望 的 曙光 和 解脱 的 迹象 .\n",
        "```\n",
        "如果设定显示翻译结果的条数（即[柱搜索算法](#柱搜索算法)的宽度）为3，生成的英语句子如下：\n",
        "```text\n",
        "0 -5.36816   These are signs of hope and relief . \u003ce\u003e\n",
        "1 -6.23177   These are the light of hope and relief . \u003ce\u003e\n",
        "2 -7.7914  These are the light of hope and the relief of hope . \u003ce\u003e\n",
        "```\n",
        "\n",
        "- 左起第一列是生成句子的序号；左起第二列是该条句子的得分（从大到小），分值越高越好；左起第三列是生成的英语句子。\n",
        "\n",
        "- 另外有两个特殊标志：`\u003ce\u003e`表示句子的结尾，`\u003cunk\u003e`表示未登录词（unknown word），即未在训练字典中出现的词。\n",
        "\n",
        "## 模型概览\n",
        "\n",
        "本节依次介绍GRU（Gated Recurrent Unit，门控循环单元），双向循环神经网络（Bi-directional Recurrent Neural Network），NMT模型中典型的编码器-解码器（Encoder-Decoder）框架和注意力（Attention）机制，以及柱搜索（beam search）算法。\n",
        "\n",
        "### GRU\n",
        "\n",
        "我们已经在[情感分析](https://github.com/PaddlePaddle/book/blob/develop/06.understand_sentiment/README.cn.md)一章中介绍了循环神经网络（RNN）及长短时间记忆网络（LSTM）。相比于简单的RNN，LSTM增加了记忆单元（memory cell）、输入门（input gate）、遗忘门（forget gate）及输出门（output gate），这些门及记忆单元组合起来大大提升了RNN处理远距离依赖问题的能力。\n",
        "\n",
        "GRU\\[[2](#参考文献)\\]是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：\n",
        "- 重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。\n",
        "- 更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。\n",
        "\u003cp align=\"center\"\u003e\n",
        "\u003cimg src=\"image/gru.png\" width=700\u003e\u003cbr/\u003e\n",
        "图2. GRU（门控循环单元）\n",
        "\u003c/p\u003e\n",
        "\n",
        "一般来说，具有短距离依赖属性的序列，其重置门比较活跃；相反，具有长距离依赖属性的序列，其更新门比较活跃。另外，Chung等人\\[[3](#参考文献)\\]通过多组实验表明，GRU虽然参数更少，但是在多个任务上都和LSTM有相近的表现。\n",
        "\n",
        "### 双向循环神经网络\n",
        "\n",
        "我们已经在[语义角色标注](https://github.com/PaddlePaddle/book/blob/develop/07.label_semantic_roles/README.cn.md)一章中介绍了一种双向循环神经网络，这里介绍Bengio团队在论文\\[[2](#参考文献),[4](#参考文献)\\]中提出的另一种结构。该结构的目的是输入一个序列，得到其在每个时刻的特征表示，即输出的每个时刻都用定长向量表示到该时刻的上下文语义信息。\n",
        "\n",
        "具体来说，该双向循环神经网络分别在时间维以顺序和逆序——即前向（forward）和后向（backward）——依次处理输入序列，并将每个时间步RNN的输出拼接成为最终的输出层。这样每个时间步的输出节点，都包含了输入序列中当前时刻完整的过去和未来的上下文信息。下图展示的是一个按时间步展开的双向循环神经网络。该网络包含一个前向和一个后向RNN，其中有六个权重矩阵：输入到前向隐层和后向隐层的权重矩阵（$W_1, W_3$），隐层到隐层自己的权重矩阵（$W_2,W_5$），前向隐层和后向隐层到输出层的权重矩阵（$W_4, W_6$）。注意，该网络的前向隐层和后向隐层之间没有连接。\n",
        "\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src = \"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/bi_rnn.png?raw=true\" width=\"400\"\u003e\u003cbr/\u003e\n",
        "图2. 按时间步展开的双向循环神经网络\n",
        "\u003c/div\u003e\n",
        "\n",
        "### 编码器-解码器框架\n",
        "\n",
        "编码器-解码器（Encoder-Decoder）\\[[2](#参考文献)\\]框架用于解决由一个任意长度的源序列到另一个任意长度的目标序列的变换问题。即编码阶段将整个源序列编码成一个向量，解码阶段通过最大化预测序列概率，从中解码出整个目标序列。编码和解码的过程通常都使用RNN实现。\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src =\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/encoder_decoder.png?raw=true\" width=\"700\"\u003e\u003cbr/\u003e\n",
        "图3. 编码器-解码器框架\n",
        "\u003c/div\u003e\n",
        "\n",
        "\u003ca name=\"编码器\"\u003e\u003c/a\u003e\n",
        "#### 编码器\n",
        "\n",
        "编码阶段分为三步：\n",
        "\n",
        "1. one-hot vector表示：将源语言句子$x=\\left \\{ x_1,x_2,...,x_T \\right \\}$的每个词$x_i$表示成一个列向量$w_i\\epsilon \\left \\{ 0,1 \\right \\}^{\\left | V \\right |},i=1,2,...,T$。这个向量$w_i$的维度与词汇表大小$\\left | V \\right |$ 相同，并且只有一个维度上有值1（该位置对应该词在词汇表中的位置），其余全是0。\n",
        "\n",
        "2. 映射到低维语义空间的词向量：one-hot vector表示存在两个问题，1）生成的向量维度往往很大，容易造成维数灾难；2）难以刻画词与词之间的关系（如语义相似性，也就是无法很好地表达语义）。因此，需再one-hot vector映射到低维的语义空间，由一个固定维度的稠密向量（称为词向量）表示。记映射矩阵为$C\\epsilon R^{K\\times \\left | V \\right |}$，用$s_i=Cw_i$表示第$i$个词的词向量，$K$为向量维度。\n",
        "\n",
        "3. 用RNN编码源语言词序列：这一过程的计算公式为$h_i=\\varnothing _\\theta \\left ( h_{i-1}, s_i \\right )$，其中$h_0$是一个全零的向量，$\\varnothing _\\theta$是一个非线性激活函数，最后得到的$\\mathbf{h}=\\left \\{ h_1,..., h_T \\right \\}$就是RNN依次读入源语言$T$个词的状态编码序列。整句话的向量表示可以采用$\\mathbf{h}$在最后一个时间步$T$的状态编码，或使用时间维上的池化（pooling）结果。\n",
        "\n",
        "第3步也可以使用双向循环神经网络实现更复杂的句编码表示，具体可以用双向GRU实现。前向GRU按照词序列$(x_1,x_2,...,x_T)$的顺序依次编码源语言端词，并得到一系列隐层状态$(\\overrightarrow{h_1},\\overrightarrow{h_2},...,\\overrightarrow{h_T})$。类似的，后向GRU按照$(x_T,x_{T-1},...,x_1)$的顺序依次编码源语言端词，得到$(\\overleftarrow{h_1},\\overleftarrow{h_2},...,\\overleftarrow{h_T})$。最后对于词$x_i$，通过拼接两个GRU的结果得到它的隐层状态，即$h_i=\\left [ \\overrightarrow{h_i^T},\\overleftarrow{h_i^T} \\right ]^{T}$。\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/encoder_attention.png?raw=true\" width=\"400\"\u003e\u003cbr/\u003e\n",
        "图4. 使用双向GRU的编码器\n",
        "\u003c/div\u003e\n",
        "\n",
        "#### 解码器\n",
        "\n",
        "机器翻译任务的训练过程中，解码阶段的目标是最大化下一个正确的目标语言词的概率。思路是：\n",
        "1. 每一个时刻，根据源语言句子的编码信息（又叫上下文向量，context vector）$c$、真实目标语言序列的第$i$个词$u_i$和$i$时刻RNN的隐层状态$z_i$，计算出下一个隐层状态$z_{i+1}$。计算公式如下：\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/decoder_formula.png?raw=true\" width=\"200\"\u003e\u003cbr/\u003e\n",
        "\u003c/div\u003e\n",
        "\n",
        "其中$\\phi _{\\theta '}$是一个非线性激活函数；$c$是源语言句子的上下文向量，在不使用注意力机制时，如果[编码器](#编码器)的输出是源语言句子编码后的最后一个元素，则可以定义$c=h_T$；$u_i$是目标语言序列的第$i$个单词，$u_0$是目标语言序列的开始标记`\u003cs\u003e`，表示解码开始；$z_i$是$i$时刻解码RNN的隐层状态，$z_0$是一个全零的向量。\n",
        "\n",
        "1. 将$z_{i+1}$通过`softmax`归一化，得到目标语言序列的第$i+1$个单词的概率分布$p_{i+1}$。概率分布公式如下：\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/probability_formula.png?raw=true\" width=\"400\"\u003e\u003cbr/\u003e\n",
        "\u003c/div\u003e\n",
        "\n",
        "其中$W_sz_{i+1}+b_z$是对每个可能的输出单词进行打分，再用softmax归一化就可以得到第$i+1$个词的概率$p_{i+1}$。\n",
        "\n",
        "1. 根据$p_{i+1}$和$u_{i+1}$计算代价。\n",
        "\n",
        "2. 重复步骤1~3，直到目标语言序列中的所有词处理完毕。\n",
        "\n",
        "机器翻译任务的生成过程，通俗来讲就是根据预先训练的模型来翻译源语言句子。生成过程中的解码阶段和上述训练过程的有所差异，具体介绍请见[柱搜索算法](#柱搜索算法)。\n",
        "\n",
        "### 注意力机制\n",
        "\n",
        "如果编码阶段的输出是一个固定维度的向量，会带来以下两个问题：1）不论源语言序列的长度是5个词还是50个词，如果都用固定维度的向量去编码其中的语义和句法结构信息，对模型来说是一个非常高的要求，特别是对长句子序列而言；2）直觉上，当人类翻译一句话时，会对与当前译文更相关的源语言片段上给予更多关注，且关注点会随着翻译的进行而改变。而固定维度的向量则相当于，任何时刻都对源语言所有信息给予了同等程度的关注，这是不合理的。因此，Bahdanau等人\\[[4](#参考文献)\\]引入注意力（attention）机制，可以对编码后的上下文片段进行解码，以此来解决长句子的特征学习问题。下面介绍在注意力机制下的解码器结构。\n",
        "\n",
        "与简单的解码器不同，这里$z_i$的计算公式为：\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/attention_decoder_formula.png?raw=true\" width=\"200\"\u003e\u003cbr/\u003e\n",
        "\u003c/div\u003e\n",
        "\n",
        "可见，源语言句子的编码向量表示为第$i$个词的上下文片段$c_i$，即针对每一个目标语言中的词$u_i$，都有一个特定的$c_i$与之对应。$c_i$的计算公式如下：\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/sum_formula.png?raw=true\" width=\"300\"\u003e\u003cbr/\u003e\n",
        "\u003c/div\u003e\n",
        "\n",
        "从公式中可以看出，注意力机制是通过对编码器中各时刻的RNN状态$h_j$进行加权平均实现的。权重$a_{ij}$表示目标语言中第$i$个词对源语言中第$j$个词的注意力大小，$a_{ij}$的计算公式如下：\n",
        "\n",
        "\u003cdiv align=\"center\"\u003e\n",
        "\u003cimg src=\"https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/image/weight_formula.png?raw=true\" width=\"300\"\u003e\u003cbr/\u003e\n",
        "\u003c/div\u003e\n",
        "\n",
        "其中，$align$可以看作是一个对齐模型，用来衡量目标语言中第$i$个词和源语言中第$j$个词的匹配程度。具体而言，这个程度是通过解码RNN的第$i$个隐层状态$z_i$和源语言句子的第$j$个上下文片段$h_j$计算得到的。传统的对齐模型中，目标语言的每个词明确对应源语言的一个或多个词（hard alignment）；而在注意力模型中采用的是soft alignment，即任何两个目标语言和源语言词间均存在一定的关联，且这个关联强度是由模型计算得到的实数，因此可以融入整个NMT框架，并通过反向传播算法进行训练。\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "\u003cimg src=\"image/decoder_attention.png\" width=500\u003e\u003cbr/\u003e\n",
        "图6. 基于注意力机制的解码器\n",
        "\u003c/p\u003e\n",
        "\n",
        "\u003ca name=\"柱搜索算法\"\u003e\u003c/a\u003e\n",
        "### 柱搜索算法\n",
        "\n",
        "柱搜索（[beam search](http://en.wikipedia.org/wiki/Beam_search)）是一种启发式图搜索算法，用于在图或树中搜索有限集合中的最优扩展节点，通常用在解空间非常大的系统（如机器翻译、语音识别）中，原因是内存无法装下图或树中所有展开的解。如在机器翻译任务中希望翻译“`\u003cs\u003e你好\u003ce\u003e`”，就算目标语言字典中只有3个词（`\u003cs\u003e`, `\u003ce\u003e`, `hello`），也可能生成无限句话（`hello`循环出现的次数不定），为了找到其中较好的翻译结果，我们可采用柱搜索算法。\n",
        "\n",
        "柱搜索算法使用广度优先策略建立搜索树，在树的每一层，按照启发代价（heuristic cost）（本教程中，为生成词的log概率之和）对节点进行排序，然后仅留下预先确定的个数（文献中通常称为beam width、beam size、柱宽度等）的节点。只有这些节点会在下一层继续扩展，其他节点就被剪掉了，也就是说保留了质量较高的节点，剪枝了质量较差的节点。因此，搜索所占用的空间和时间大幅减少，但缺点是无法保证一定获得最优解。\n",
        "\n",
        "使用柱搜索算法的解码阶段，目标是最大化生成序列的概率。思路是：\n",
        "1. 每一个时刻，根据源语言句子的编码信息$c$、生成的第$i$个目标语言序列单词$u_i$和$i$时刻RNN的隐层状态$z_i$，计算出下一个隐层状态$z_{i+1}$。\n",
        "\n",
        "2. 将$z_{i+1}$通过`softmax`归一化，得到目标语言序列的第$i+1$个单词的概率分布$p_{i+1}$。\n",
        "\n",
        "3. 根据$p_{i+1}$采样出单词$u_{i+1}$。\n",
        "\n",
        "4. 重复步骤1~3，直到获得句子结束标记`\u003ce\u003e`或超过句子的最大生成长度为止。\n",
        "\n",
        "注意：$z_{i+1}$和$p_{i+1}$的计算公式同[解码器](#解码器)中的一样。且由于生成时的每一步都是通过贪心法实现的，因此并不能保证得到全局最优解。\n",
        "\n",
        "## 数据介绍\n",
        "\n",
        "本教程使用[WMT-16](http://www.statmt.org/wmt16/)新增的[multimodal task](http://www.statmt.org/wmt16/multimodal-task.html)中的[translation task](http://www.statmt.org/wmt16/multimodal-task.html#task1)的数据集。该数据集为英德翻译数据，包含29001条训练数据，1000条测试数据。\n",
        "\n",
        "### 数据预处理\n",
        "\n",
        "我们的预处理流程包括两步：\n",
        "\n",
        "- 将每个源语言到目标语言的平行语料库文件合并为一个文件：\n",
        "\n",
        "- 合并每个`XXX.src`和`XXX.trg`文件为`XXX`。\n",
        "\n",
        "- `XXX`中的第$i$行内容为`XXX.src`中的第$i$行和`XXX.trg`中的第$i$行连接，用'\\t'分隔。\n",
        "\n",
        "- 创建训练数据的“源字典”和“目标字典”。每个字典都有**DICTSIZE**个单词，包括：语料中词频最高的（DICTSIZE - 3）个单词，和3个特殊符号`\u003cs\u003e`（序列的开始）、`\u003ce\u003e`（序列的结束）和`\u003cunk\u003e`（未登录词）。\n",
        "\n",
        "### 示例数据\n",
        "\n",
        "为了验证训练流程，PaddlePaddle接口`paddle.dataset.wmt16`中提供了对该数据集[预处理后的版本](http://paddlemodels.bj.bcebos.com/wmt/wmt16.tar.gz)，调用该接口即可直接使用，因为数据规模限制，这里只作为示例使用，在相应的测试集上具有一定效果但在更多测试数据上的效果无法保证。\n",
        "\n",
        "## 模型配置说明\n",
        "\n",
        "下面我们开始根据输入数据的形式配置模型。首先引入所需的库函数以及定义全局变量：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import six\n",
        "\n",
        "import paddle\n",
        "import paddle.fluid as fluid\n",
        "\n",
        "dict_size = 30000  # 词典大小\n",
        "source_dict_size = target_dict_size = dict_size  # 源/目标语言字典大小\n",
        "word_dim = 512  # 词向量维度\n",
        "hidden_dim = 512  # 编码器中的隐层大小\n",
        "decoder_size = hidden_dim  # 解码器中的隐层大小\n",
        "max_length = 256  # 解码生成句子的最大长度\n",
        "beam_size = 4  # beam search的柱宽度\n",
        "batch_size = 64  # batch 中的样本数\n",
        "\n",
        "is_sparse = True\n",
        "model_save_dir = \"machine_translation.inference.model\"\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "然后如下实现编码器框架：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def encoder():\n",
        "    # 定义源语言id序列的输入数据\n",
        "    src_word_id = fluid.layers.data(\n",
        "        name=\"src_word_id\", shape=[1], dtype='int64', lod_level=1)\n",
        "    # 将上述编码映射到低维语言空间的词向量\n",
        "    src_embedding = fluid.layers.embedding(\n",
        "        input=src_word_id,\n",
        "        size=[source_dict_size, word_dim],\n",
        "        dtype='float32',\n",
        "        is_sparse=is_sparse)\n",
        "    # 用双向GRU编码源语言序列，拼接两个GRU的编码结果得到h\n",
        "    fc_forward = fluid.layers.fc(\n",
        "        input=src_embedding, size=hidden_dim * 3, bias_attr=False)\n",
        "    src_forward = fluid.layers.dynamic_gru(input=fc_forward, size=hidden_dim)\n",
        "    fc_backward = fluid.layers.fc(\n",
        "        input=src_embedding, size=hidden_dim * 3, bias_attr=False)\n",
        "    src_backward = fluid.layers.dynamic_gru(\n",
        "        input=fc_backward, size=hidden_dim, is_reverse=True)\n",
        "    encoded_vector = fluid.layers.concat(\n",
        "        input=[src_forward, src_backward], axis=1)\n",
        "    return encoded_vector\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "再实现基于注意力机制的解码器：\n",
        "  - 首先定义解码器中单步的计算，即$z_{i+1}=\\phi _{\\theta '}\\left ( c_i,u_i,z_i \\right )$，如下：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    # 定义RNN中的单步计算\n",
        "    def cell(x, hidden, encoder_out, encoder_out_proj):\n",
        "        # 定义attention用以计算context，即 c_i，这里使用Bahdanau attention机制\n",
        "        def simple_attention(encoder_vec, encoder_proj, decoder_state):\n",
        "            decoder_state_proj = fluid.layers.fc(\n",
        "                input=decoder_state, size=decoder_size, bias_attr=False)\n",
        "            # sequence_expand将单步内容扩展为与encoder输出相同的序列\n",
        "            decoder_state_expand = fluid.layers.sequence_expand(\n",
        "                x=decoder_state_proj, y=encoder_proj)\n",
        "            mixed_state = fluid.layers.elementwise_add(encoder_proj,\n",
        "                                                    decoder_state_expand)\n",
        "            attention_weights = fluid.layers.fc(\n",
        "                input=mixed_state, size=1, bias_attr=False)\n",
        "            attention_weights = fluid.layers.sequence_softmax(\n",
        "                input=attention_weights)\n",
        "            weigths_reshape = fluid.layers.reshape(x=attention_weights, shape=[-1])\n",
        "            scaled = fluid.layers.elementwise_mul(\n",
        "                x=encoder_vec, y=weigths_reshape, axis=0)\n",
        "            context = fluid.layers.sequence_pool(input=scaled, pool_type='sum')\n",
        "            return context\n",
        "\n",
        "        context = simple_attention(encoder_out, encoder_out_proj, hidden)\n",
        "        out = fluid.layers.fc(\n",
        "            input=[x, context], size=decoder_size * 3, bias_attr=False)\n",
        "        out = fluid.layers.gru_unit(\n",
        "            input=out, hidden=hidden, size=decoder_size * 3)[0]\n",
        "        return out, out\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "  - 基于定义的单步计算，使用`DynamicRNN`实现多步循环的训练模式下解码器，如下：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    def train_decoder(encoder_out):\n",
        "        # 获取编码器输出的最后一步并进行非线性映射以构造解码器RNN的初始状态\n",
        "        encoder_last = fluid.layers.sequence_last_step(input=encoder_out)\n",
        "        encoder_last_proj = fluid.layers.fc(\n",
        "            input=encoder_last, size=decoder_size, act='tanh')\n",
        "        # 编码器输出在attention中计算结果的cache\n",
        "        encoder_out_proj = fluid.layers.fc(\n",
        "            input=encoder_out, size=decoder_size, bias_attr=False)\n",
        "\n",
        "        # 定义目标语言id序列的输入数据，并映射到低维语言空间的词向量\n",
        "        trg_language_word = fluid.layers.data(\n",
        "            name=\"target_language_word\", shape=[1], dtype='int64', lod_level=1)\n",
        "        trg_embedding = fluid.layers.embedding(\n",
        "            input=trg_language_word,\n",
        "            size=[target_dict_size, word_dim],\n",
        "            dtype='float32',\n",
        "            is_sparse=is_sparse)\n",
        "\n",
        "        rnn = fluid.layers.DynamicRNN()\n",
        "        with rnn.block():\n",
        "            # 获取当前步目标语言输入的词向量\n",
        "            x = rnn.step_input(trg_embedding)\n",
        "            # 获取隐层状态\n",
        "            pre_state = rnn.memory(init=encoder_last_proj, need_reorder=True)\n",
        "            # 在DynamicRNN中需使用static_input获取encoder相关的内容\n",
        "            # 对decoder来说这些内容在每个时间步都是固定的\n",
        "            encoder_out = rnn.static_input(encoder_out)\n",
        "            encoder_out_proj = rnn.static_input(encoder_out_proj)\n",
        "            # 执行单步的计算单元\n",
        "            out, current_state = cell(x, pre_state, encoder_out, encoder_out_proj)\n",
        "            # 计算归一化的单词预测概率\n",
        "            prob = fluid.layers.fc(input=out, size=target_dict_size, act='softmax')\n",
        "            # 更新隐层状态\n",
        "            rnn.update_memory(pre_state, current_state)\n",
        "            # 输出预测概率\n",
        "            rnn.output(prob)\n",
        "\n",
        "        return rnn()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "接着就可以使用编码器和解码器定义整个训练网络；为了进行训练还需要定义优化器，如下：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def train_model():\n",
        "    encoder_out = encoder()\n",
        "    rnn_out = train_decoder(encoder_out)\n",
        "    label = fluid.layers.data(\n",
        "        name=\"target_language_next_word\", shape=[1], dtype='int64', lod_level=1)\n",
        "    # 定义损失函数\n",
        "    cost = fluid.layers.cross_entropy(input=rnn_out, label=label)\n",
        "    avg_cost = fluid.layers.mean(cost)\n",
        "    return avg_cost\n",
        "\n",
        "def optimizer_func():\n",
        "    # 设置梯度裁剪\n",
        "    fluid.clip.set_gradient_clip(\n",
        "        clip=fluid.clip.GradientClipByGlobalNorm(clip_norm=5.0))\n",
        "    # 定义先增后降的学习率策略\n",
        "    lr_decay = fluid.layers.learning_rate_scheduler.noam_decay(hidden_dim, 1000)\n",
        "    return fluid.optimizer.Adam(\n",
        "        learning_rate=lr_decay,\n",
        "        regularization=fluid.regularizer.L2DecayRegularizer(\n",
        "            regularization_coeff=1e-4))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "以上是训练所需的模型构件，预测（生成）模式下基于beam search的解码器需要借助`while_op`实现，如下：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def infer_decoder(encoder_out):\n",
        "    # 获取编码器输出的最后一步并进行非线性映射以构造解码器RNN的初始状态\n",
        "    encoder_last = fluid.layers.sequence_last_step(input=encoder_out)\n",
        "    encoder_last_proj = fluid.layers.fc(\n",
        "        input=encoder_last, size=decoder_size, act='tanh')\n",
        "    # 编码器输出在attention中计算结果的cache\n",
        "    encoder_out_proj = fluid.layers.fc(\n",
        "        input=encoder_out, size=decoder_size, bias_attr=False)\n",
        "\n",
        "    # 最大解码步数\n",
        "    max_len = fluid.layers.fill_constant(\n",
        "        shape=[1], dtype='int64', value=max_length)\n",
        "    # 解码步数计数变量\n",
        "    counter = fluid.layers.zeros(shape=[1], dtype='int64', force_cpu=True)\n",
        "\n",
        "    # 定义 tensor array 用以保存各个时间步的内容，并写入初始id，score和state\n",
        "    init_ids = fluid.layers.data(\n",
        "        name=\"init_ids\", shape=[1], dtype=\"int64\", lod_level=2)\n",
        "    init_scores = fluid.layers.data(\n",
        "        name=\"init_scores\", shape=[1], dtype=\"float32\", lod_level=2)\n",
        "    ids_array = fluid.layers.array_write(init_ids, i=counter)\n",
        "    scores_array = fluid.layers.array_write(init_scores, i=counter)\n",
        "    state_array = fluid.layers.array_write(encoder_last_proj, i=counter)\n",
        "\n",
        "    # 定义循环终止条件变量\n",
        "    cond = fluid.layers.less_than(x=counter, y=max_len)\n",
        "    while_op = fluid.layers.While(cond=cond)\n",
        "    with while_op.block():\n",
        "        # 获取解码器在当前步的输入，包括上一步选择的id，对应的score和上一步的state\n",
        "        pre_ids = fluid.layers.array_read(array=ids_array, i=counter)\n",
        "        pre_score = fluid.layers.array_read(array=scores_array, i=counter)\n",
        "        pre_state = fluid.layers.array_read(array=state_array, i=counter)\n",
        "\n",
        "        # 同train_decoder中的内容，进行RNN的单步计算\n",
        "        pre_ids_emb = fluid.layers.embedding(\n",
        "            input=pre_ids,\n",
        "            size=[target_dict_size, word_dim],\n",
        "            dtype='float32',\n",
        "            is_sparse=is_sparse)\n",
        "        out, current_state = cell(pre_ids_emb, pre_state, encoder_out,\n",
        "                            encoder_out_proj)\n",
        "        prob = fluid.layers.fc(\n",
        "            input=current_state, size=target_dict_size, act='softmax')\n",
        "\n",
        "        # 计算累计得分，进行beam search\n",
        "        topk_scores, topk_indices = fluid.layers.topk(prob, k=beam_size)\n",
        "        accu_scores = fluid.layers.elementwise_add(\n",
        "            x=fluid.layers.log(topk_scores),\n",
        "            y=fluid.layers.reshape(pre_score, shape=[-1]),\n",
        "            axis=0)\n",
        "        accu_scores = fluid.layers.lod_reset(x=accu_scores, y=pre_ids)\n",
        "        selected_ids, selected_scores = fluid.layers.beam_search(\n",
        "            pre_ids, pre_score, topk_indices, accu_scores, beam_size, end_id=1)\n",
        "\n",
        "        fluid.layers.increment(x=counter, value=1, in_place=True)\n",
        "        # 将 search 结果写入 tensor array 中\n",
        "        fluid.layers.array_write(selected_ids, array=ids_array, i=counter)\n",
        "        fluid.layers.array_write(selected_scores, array=scores_array, i=counter)\n",
        "        # sequence_expand 作为 gather 使用以获取search结果对应的状态，并更新\n",
        "        current_state = fluid.layers.sequence_expand(current_state,\n",
        "                                                     selected_ids)\n",
        "        fluid.layers.array_write(current_state, array=state_array, i=counter)\n",
        "        current_enc_out = fluid.layers.sequence_expand(encoder_out,\n",
        "                                                       selected_ids)\n",
        "        fluid.layers.assign(current_enc_out, encoder_out)\n",
        "        current_enc_out_proj = fluid.layers.sequence_expand(\n",
        "            encoder_out_proj, selected_ids)\n",
        "        fluid.layers.assign(current_enc_out_proj, encoder_out_proj)\n",
        "\n",
        "        # 更新循环终止条件\n",
        "        length_cond = fluid.layers.less_than(x=counter, y=max_len)\n",
        "        finish_cond = fluid.layers.logical_not(\n",
        "            fluid.layers.is_empty(x=selected_ids))\n",
        "        fluid.layers.logical_and(x=length_cond, y=finish_cond, out=cond)\n",
        "\n",
        "    # 根据保存的每一步的结果，回溯生成最终解码结果\n",
        "    translation_ids, translation_scores = fluid.layers.beam_search_decode(\n",
        "        ids=ids_array, scores=scores_array, beam_size=beam_size, end_id=1)\n",
        "\n",
        "    return translation_ids, translation_scores\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "使用编码器和预测模式的解码器，预测网络定义如下：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "def infer_model():\n",
        "    encoder_out = encoder()\n",
        "    translation_ids, translation_scores = infer_decoder(encoder_out)\n",
        "    return translation_ids, translation_scores\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 训练模型\n",
        "\n",
        "### 构建训练程序\n",
        "\n",
        "定义用于训练的`Program`，在其中创建训练的网络结构并添加优化器。同时还要定义用于初始化的`Program`，在创建训练网络的同时隐式的加入参数初始化的操作。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "train_prog = fluid.Program()\n",
        "startup_prog = fluid.Program()\n",
        "with fluid.program_guard(train_prog, startup_prog):\n",
        "    with fluid.unique_name.guard():\n",
        "        avg_cost = train_model()\n",
        "        optimizer = optimizer_func()\n",
        "        optimizer.minimize(avg_cost)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 定义训练环境与执行器\n",
        "\n",
        "定义您的训练环境，可以指定训练是发生在CPU还是GPU上；并基于这个训练环境定义执行器。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "use_cuda = False\n",
        "# 定义使用设备和执行器\n",
        "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
        "exe = fluid.Executor(place)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 构建数据提供器\n",
        "\n",
        "使用封装的`paddle.dataset.wmt16.train`接口定义数据生成器，其每次产生一条样本，shuffle和组完batch后作为训练的输入；另外还需要指明输入数据中各字段和`data_layer`定义的各输入的对应关系，这可以通过`DataFeeder`完成, 下面的feeder将产生数据的第一列映射到`src_word_id`这个输入。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# 定义训练数据生成器\n",
        "train_data = paddle.batch(\n",
        "    paddle.reader.shuffle(\n",
        "        paddle.dataset.wmt16.train(source_dict_size, target_dict_size),\n",
        "        buf_size=10000),\n",
        "    batch_size=batch_size)\n",
        "# DataFeeder完成\n",
        "feeder = fluid.DataFeeder(\n",
        "    feed_list=[\n",
        "        'src_word_id', 'target_language_word', 'target_language_next_word'\n",
        "    ],\n",
        "    place=place,\n",
        "    program=train_prog)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 训练主循环\n",
        "\n",
        "通过训练循环数（EPOCH_NUM）来进行训练循环，并且每次循环都保存训练好的参数。注意，循环训练前要首先执行初始化的`Program`来初始化参数。另外作为示例这里EPOCH_NUM设置较小，该数据集上实际大概需要20\u0008个epoch左右收敛。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# 执行初始化 Program，进行参数初始化\n",
        "exe.run(startup_prog)\n",
        "# 循环迭代执行训练\n",
        "EPOCH_NUM = 2\n",
        "for pass_id in six.moves.xrange(EPOCH_NUM):\n",
        "    batch_id = 0\n",
        "    for data in train_data():\n",
        "        cost = exe.run(\n",
        "            train_prog, feed=feeder.feed(data), fetch_list=[avg_cost])[0]\n",
        "        print('pass_id: %d, batch_id: %d, loss: %f' % (pass_id, batch_id,\n",
        "                                                        cost))\n",
        "        batch_id += 1\n",
        "    # 保存模型\n",
        "    fluid.io.save_params(exe, model_save_dir, main_program=train_prog)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 应用模型\n",
        "\n",
        "### 构建预测程序\n",
        "\n",
        "定义用于预测的`Program`，在其中创建预测的网络结构。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "infer_prog = fluid.Program()\n",
        "startup_prog = fluid.Program()\n",
        "with fluid.program_guard(infer_prog, startup_prog):\n",
        "    with fluid.unique_name.guard():\n",
        "        translation_ids, translation_scores = infer_model()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 构建数据提供器\n",
        "\n",
        "和训练类似，这里使用封装的`paddle.dataset.wmt16.test`接口定义测试数据生成器，测试数据共1000条，组完batch后作为预测的输入；另外我们获取源语言和目标语言id到word的词典，以将id序列转换为明文序列打印输出。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "test_data = paddle.batch(\n",
        "    paddle.dataset.wmt16.test(source_dict_size, target_dict_size),\n",
        "    batch_size=batch_size)\n",
        "src_idx2word = paddle.dataset.wmt16.get_dict(\n",
        "    \"en\", source_dict_size, reverse=True)\n",
        "trg_idx2word = paddle.dataset.wmt16.get_dict(\n",
        "    \"de\", target_dict_size, reverse=True)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 测试\n",
        "首先要加载训练过程保存下来的模型，然后就可以循环测试数据进行预测了。这里每次运行我们都会创建`data_layer`对应输入数据的`dict`传入，这个和`DataFeeder`相同的效果。生成过程对于每个测试数据都会将源语言句子和`beam_size`个生成句子打印输出。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "    fluid.io.load_params(exe, model_save_dir, main_program=infer_prog)\n",
        "\n",
        "    for data in test_data():\n",
        "        src_word_id = fluid.create_lod_tensor(\n",
        "            data=[x[0] for x in data],\n",
        "            recursive_seq_lens=[[len(x[0]) for x in data]],\n",
        "            place=place)\n",
        "        # init_ids内容为start token\n",
        "        init_ids = fluid.create_lod_tensor(\n",
        "            data=np.array([[0]] * len(data), dtype='int64'),\n",
        "            recursive_seq_lens=[[1] * len(data)] * 2,\n",
        "            place=place)\n",
        "        # init_scores为beam search过程累积得分的初值\n",
        "        init_scores = fluid.create_lod_tensor(\n",
        "            data=np.array([[0.]] * len(data), dtype='float32'),\n",
        "            recursive_seq_lens=[[1] * len(data)] * 2,\n",
        "            place=place)\n",
        "        seq_ids, seq_scores = exe.run(\n",
        "            infer_prog,\n",
        "            feed={\n",
        "                'src_word_id': src_word_id,\n",
        "                'init_ids': init_ids,\n",
        "                'init_scores': init_scores\n",
        "            },\n",
        "            fetch_list=[translation_ids, translation_scores],\n",
        "            return_numpy=False)\n",
        "        # 如何解析翻译结果详见 train.py 中对应代码的注释说明\n",
        "        hyps = [[] for i in range(len(seq_ids.lod()[0]) - 1)]\n",
        "        scores = [[] for i in range(len(seq_scores.lod()[0]) - 1)]\n",
        "        for i in range(len(seq_ids.lod()[0]) - 1):\n",
        "            start = seq_ids.lod()[0][i]\n",
        "            end = seq_ids.lod()[0][i + 1]\n",
        "            print(\"Original sentence:\")\n",
        "            print(\" \".join([src_idx2word[idx] for idx in data[i][0][1:-1]]))\n",
        "            print(\"Translated score and sentence:\")\n",
        "            for j in range(end - start):\n",
        "                sub_start = seq_ids.lod()[1][start + j]\n",
        "                sub_end = seq_ids.lod()[1][start + j + 1]\n",
        "                hyps[i].append(\" \".join([\n",
        "                    trg_idx2word[idx]\n",
        "                    for idx in np.array(seq_ids)[sub_start:sub_end][1:-1]\n",
        "                ]))\n",
        "                scores[i].append(np.array(seq_scores)[sub_end - 1])\n",
        "                print(scores[i][-1], hyps[i][-1].encode('utf8'))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "可以观察到如下的预测结果输出：\n",
        "```txt\n",
        "Original sentence:\n",
        "Two adults and two children sit on a park bench .\n",
        "Translated score and sentence:\n",
        "-2.5993705 Zwei Erwachsene und zwei Kinder sitzen auf einer Parkbank .\n",
        "-2.6617606 Zwei Erwachsene und zwei Kinder spielen auf einer Parkbank .\n",
        "-3.186554 Zwei Erwachsene und zwei Kinder sitzen auf einer Bank .\n",
        "-3.4353821 Zwei Erwachsene und zwei Kinder spielen auf einer Bank .\n",
        "```\n",
        "\n",
        "## 总结\n",
        "\n",
        "端到端的神经网络机器翻译是近几年兴起的一种全新的机器翻译方法。本章中，我们介绍了NMT中典型的“编码器-解码器”框架。由于NMT是一个典型的Seq2Seq（Sequence to Sequence，序列到序列）学习问题，因此，Seq2Seq中的query改写（query rewriting）、摘要、单轮对话等问题都可以用本教程的模型来解决。\n",
        "\n",
        "\u003ca name=\"参考文献\"\u003e\u003c/a\u003e\n",
        "## 参考文献\n",
        "\n",
        "1. Koehn P. [Statistical machine translation](https://books.google.com.hk/books?id=4v_Cx1wIMLkC\u0026printsec=frontcover\u0026hl=zh-CN\u0026source=gbs_ge_summary_r\u0026cad=0#v=onepage\u0026q\u0026f=false)[M]. Cambridge University Press, 2009.\n",
        "2. Cho K, Van Merriënboer B, Gulcehre C, et al. [Learning phrase representations using RNN encoder-decoder for statistical machine translation](http://www.aclweb.org/anthology/D/D14/D14-1179.pdf)[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014: 1724-1734.\n",
        "3. Chung J, Gulcehre C, Cho K H, et al. [Empirical evaluation of gated recurrent neural networks on sequence modeling](https://arxiv.org/abs/1412.3555)[J]. arXiv preprint arXiv:1412.3555, 2014.\n",
        "4.  Bahdanau D, Cho K, Bengio Y. [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)[C]//Proceedings of ICLR 2015, 2015.\n",
        "5. Papineni K, Roukos S, Ward T, et al. [BLEU: a method for automatic evaluation of machine translation](http://dl.acm.org/citation.cfm?id=1073135)[C]//Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002: 311-318.\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"\u003e\u003cimg alt=\"知识共享许可协议\" style=\"border-width:0\" src=\"https://paddlepaddleimage.cdn.bcebos.com/bookimage/camo.png\" /\u003e\u003c/a\u003e\u003cbr /\u003e\u003cspan xmlns:dct=\"http://purl.org/dc/terms/\" href=\"http://purl.org/dc/dcmitype/Text\" property=\"dct:title\" rel=\"dct:type\"\u003e本教程\u003c/span\u003e 由 \u003ca xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://book.paddlepaddle.org\" property=\"cc:attributionName\" rel=\"cc:attributionURL\"\u003ePaddlePaddle\u003c/a\u003e 创作，采用 \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"\u003e知识共享 署名-相同方式共享 4.0 国际 许可协议\u003c/a\u003e进行许可。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
